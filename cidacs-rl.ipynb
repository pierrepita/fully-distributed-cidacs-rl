{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import jellyfish\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option(\"display.max_columns\", 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CIDACSRL\") \\\n",
    "    .master(\"spark://barravento:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.elasticsearch:elasticsearch-spark-30_2.12:8.1.3\") \\\n",
    "    .config(\"spark.es.nodes\", \"barravento\") \\\n",
    "    .config(\"spark.es.port\", \"9200\") \\\n",
    "    .config(\"spark.es.nodes.wan.only\", \"false\") \\\n",
    "    .config(\"spark.es.resource\", \"dbb2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ES functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_match_cols_and_values(vars_col, query_type, add_id_col):\n",
    "    \"\"\"\n",
    "    query_type must be 'exact' for building exact queries or 'general' for any else query and comparison.\n",
    "    \"\"\"\n",
    "    config_ = config_bc.value\n",
    "    # getting names of indexed columns\n",
    "    indexed_id_column = config_['datasets_info']['indexed_dataset']['id_column_name']\n",
    "    \n",
    "    indexed_cols = config_['datasets_info']['indexed_dataset']['columns']\n",
    "    \n",
    "#     if query_type == 'general':\n",
    "#         indexed_cols = [x for x in indexed_cols if x != indexed_id_column]\n",
    "        \n",
    "    # notice that we are linking indexed keys with tolink values\n",
    "    # the keys will be used to set which field will be fetched on es\n",
    "    # the values will be used as search content\n",
    "    tolink_cols_dict = dict(zip(indexed_cols, vars_col))\n",
    "    \n",
    "    if add_id_col == False:\n",
    "        tolink_cols_dict.pop(indexed_id_column, None)\n",
    "    \n",
    "    if query_type == 'general':\n",
    "        return tolink_cols_dict\n",
    "    elif query_type == 'exact':\n",
    "        # finding which are the columns used on exact match step\n",
    "        indexed_exact_match_vars = [indexed_id_column] + [config_['comparisons'][x]['indexed_col'] for x in config_['comparisons'] if config_['comparisons'][x]['must_match'] == 'true']\n",
    "        non_exact_match_cols = list(set(indexed_cols) - set(indexed_exact_match_vars))\n",
    "        # deleting those columns of non-exact match\n",
    "        [tolink_cols_dict.pop(x, None) for x in non_exact_match_cols]\n",
    "        \n",
    "        return tolink_cols_dict\n",
    "    else: \n",
    "        print(\"Please use 'general' or 'exact' as query_type input\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def index_dataframe(dataframe, es_index_name):\n",
    "    # creating new index\n",
    "    dataframe.write.format(\"org.elasticsearch.spark.sql\") \\\n",
    "                 .option(\"es.resource\", es_index_name).mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_exact_queries(vars_col): \n",
    "    \"\"\"\n",
    "    Let us suppose the following values:\n",
    "    vars_col = ['ROBESPIERRE PITA', '1987-05-05', '1', 'Mari Santos']\n",
    "    indexed_cols = ['name', 'birthdate', 'sex', 'mothers_name']\n",
    "    query_size = 10\n",
    "    \n",
    "    and only the first two attributes are assigned to exact match.\n",
    "    So, the resulting query column would be: \n",
    "    '{ \"size\": \"50\", \"query\": \n",
    "                    { \"bool\": { \"must\": [ \n",
    "                                {\"match\": {\"name\":\"ROBESPIERRE PITA\"}},\n",
    "                                {\"match\": {\"birthdate\":\"19870505\"}}] } } }'\n",
    "    Requirements: \n",
    "    - All values on vars_col must be converted into string\n",
    "    - All the hyphens symbols must be taken from date type used to search (e.g. 1987-05-05 must be converted to 19870505)\n",
    "    - The config json must be available as a broadcast through sc.broadcast() function.\n",
    "    - The names of indexed columns must be correctly filled. \n",
    "    \"\"\"\n",
    "    config_ = config_bc.value\n",
    "    query_size = config_['query_size']\n",
    "    \n",
    "    tolink_cols_dict = get_match_cols_and_values(vars_col, 'exact', False)\n",
    "    \n",
    "    # -------------------------------------------- #\n",
    "    #   starting the building of query string      #\n",
    "    # -------------------------------------------- #\n",
    "    # setting the preffix and suffix of query core\n",
    "    prefix_ = \"\"\"{\"match\": {\"\"\"\n",
    "    suffix_ = \"\"\"}}\"\"\"\n",
    "    \n",
    "    # filling the query core with all indexed columns and values from vars_col\n",
    "    strings = []\n",
    "    for col in list(tolink_cols_dict.keys()):\n",
    "        string = str(prefix_) + \"\\\"\" + str(col) + \"\\\"\" + \":\" + \"\\\"\" +  str(tolink_cols_dict[col]) + \"\\\"\" + str(suffix_)\n",
    "        strings.append(string)\n",
    "    \n",
    "    # building the query core. \n",
    "    # Should be like: {\"match\": {\"name\":\"ROBESPIERRE PITA\"}}, {\"birthdate\": {\"name\":\"1987-05-05\"}}\n",
    "    line = ','.join(strings)\n",
    "    \n",
    "    # Finally the final query string\n",
    "    complete_query = \"\"\"{ \"bool\": { \"must\": [ %s ] } }\"\"\" % (line)\n",
    "    # CHANGELOG: to accomplish with new syntax from ES 8.x, we need to change the complete query from\n",
    "    # complete_query = \"\"\"{ \"size\": \"%s\", \"query\": { \"bool\": { \"must\": [ %s ] } } }\"\"\" % (query_size,line)\n",
    "    # to\n",
    "    # complete_query = \"\"\"{ \"bool\": { \"must\": [ %s ] } } }\"\"\" % (line)\n",
    "    # read more in: https://github.com/elastic/elasticsearch-py/issues/1698\n",
    "    #               https://www.elastic.co/guide/en/elasticsearch/client/python-api/8.1/examples.html\n",
    "    \n",
    "    return complete_query\n",
    "udf_build_exact_queries = F.udf(build_exact_queries, StringType()) \n",
    "\n",
    "def build_non_exact_queries(vars_col): \n",
    "    \"\"\"\n",
    "    Let us suppose the following values:\n",
    "    vars_col = ['ROBESPIERRE PITA', '1987-05-05', '1', 'Mari Santos']\n",
    "    indexed_cols = ['name', 'birthdate', 'sex', 'mothers_name']\n",
    "    query_size = 10\n",
    "    \n",
    "    and only the first two attributes are assigned to exact match.\n",
    "    So, the resulting query column would be: \n",
    "    '{\"bool\": { \n",
    "                 \"should\": [ \n",
    "                     {'match': {'nome_a': {'query': 'ROBESPIERRE PITA', 'fuzziness':'AUTO', 'operator':'or', 'boost':'3.0'}}},\n",
    "                     {\"match\": {\"birthdate\":\"19870505\"}} ] } } }\n",
    "                     {\"term\": {\"sexo_a\":\"1\"}} ] } } }'\n",
    "    Requirements: \n",
    "    - All values on vars_col must be converted into string\n",
    "    - All the hyphens symbols must be taken from date type used to search (e.g. 1987-05-05 must be converted to 19870505)\n",
    "    - The config json must be available as a broadcast through sc.broadcast() function.\n",
    "    - The names of indexed columns must be correctly filled. \n",
    "    \"\"\"\n",
    "    config_ = config_bc.value\n",
    "    query_size = config_['query_size']\n",
    "    \n",
    "    tolink_cols_dict = get_match_cols_and_values(vars_col, 'exact', False)\n",
    "    \n",
    "    # -------------------------------------------- #\n",
    "    #   starting the building of query string      #\n",
    "    # -------------------------------------------- #\n",
    "    \n",
    "    # filling the query core with all indexed columns and values from vars_col\n",
    "    comparisons = [config_['comparisons'][x] for x in config_['comparisons']]\n",
    "    strings = []\n",
    "    for col in list(tolink_cols_dict.keys()):\n",
    "        query_col_instructions = [x for x in comparisons if x['indexed_col'] == col][0]\n",
    "        query_type = str(query_col_instructions['query_type'])\n",
    "        prefix_ = \"\"\"{\"%s\": {\"\"\" % query_type\n",
    "        suffix_ = \"\"\"}}\"\"\"\n",
    "\n",
    "        if query_col_instructions['should_match'] == 'true':\n",
    "            if query_col_instructions['is_fuzzy'] == 'true':\n",
    "                boost = str(query_col_instructions['boost'])\n",
    "                string = str(prefix_) + \"\\\"\" + str(col) + \"\\\"\" + \":\" + \" { \\\"query\\\" : \\\"\" +  str(tolink_cols_dict[col]) + \"\\\"\" + \", \\\"fuzziness\\\":\\\"AUTO\\\", \\\"operator\\\":\\\"or\\\", \\\"boost\\\":\\\"\" + boost + \"\\\" }\" + str(suffix_)\n",
    "                \n",
    "            if query_col_instructions['is_fuzzy'] == 'false':\n",
    "                string = str(prefix_) + \"\\\"\" + str(col) + \"\\\"\" + \":\" + \"\\\"\" +  str(tolink_cols_dict[col]) + \"\\\"\" + str(suffix_)\n",
    "        strings.append(string)\n",
    "    \n",
    "    # building the query core. \n",
    "    # is_fuzzy = 'true' should be like: {\"match\": {\"name\":\"ROBESPIERRE PITA\", \"fuzziness\":\"AUTO\", \"operator\":\"or\", \"boost\":\"3.0\"}}, {\"term\": {\"dt_nasc_a\":\"20070816\"}}\n",
    "    line = ','.join(strings)\n",
    "    \n",
    "    # Finally the final query string\n",
    "    complete_query = \"\"\"{ \"bool\": { \"should\": [ %s ] } }\"\"\" % (line)\n",
    "    # CHANGELOG: to accomplish with new syntax from ES 8.x, we need to change the complete query from\n",
    "    # complete_query = \"\"\"{ \"size\": \"%s\", \"query\": { \"bool\": { \"should\": [ %s ] } } }\"\"\" % (query_size,line)\n",
    "    # to\n",
    "    # complete_query = \"\"\"{ \"bool\": { \"should\": [ %s ] } } }\"\"\" % (query_size,line)\n",
    "    # read more in: https://github.com/elastic/elasticsearch-py/issues/1698\n",
    "    #               https://www.elastic.co/guide/en/elasticsearch/client/python-api/8.1/examples.html\n",
    "    \n",
    "    return complete_query\n",
    "udf_build_non_exact_queries = F.udf(build_non_exact_queries, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finding matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_elasticsearch_exact_best_candidate(vars_col, exact_queries_col):\n",
    "    \"\"\"\n",
    "    Let us suppose a column with the following query:\n",
    "    \n",
    "    '{ \"bool\": { \"must\": [ \n",
    "                    {\"match\": {\"name\":\"ROBESPIERRE PITA\"}},\n",
    "                    {\"match\": {\"birthdate\":\"19870505\"}}] } }'\n",
    "    \n",
    "    so, this function must return a dict with N results like: \n",
    "        {'_index': 'test', '_type': '_doc', '_id': 'aaabbbccc', '_score': 43.9280841,\n",
    "        '_source': {'name': 'ROBESPIERRE PITA', 'birthdate': '19870505', 'other_col': 'other_value'}},\n",
    "    \n",
    "    being N the query_size value set on config, you can see this number on the 'size' field of the query.\n",
    "    \n",
    "    This result can now be used to compute the proper similarity and pick the \n",
    "    best candidate for each record\n",
    "    \"\"\"\n",
    "    from elasticsearch import Elasticsearch\n",
    "    from ast import literal_eval\n",
    "\n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    es_connect_string = config_['es_connect_string']\n",
    "    es_index_name = config_['es_index_name']\n",
    "    query_size = config_['query_size']\n",
    "    \n",
    "    es = Elasticsearch(es_connect_string)\n",
    "    \n",
    "    exact_queries_col = literal_eval(exact_queries_col)\n",
    "    # CHANGELOG: To accomplish with the syntax of ES 8.x we need to add the lines\n",
    "    # from ast import literal_eval\n",
    "    # exact_queries_col = literal_eval(exact_queries_col)\n",
    "    # String typed query contents does not meet the new requisits, rainsing the following error (even when they are properly written): \n",
    "    # # elasticsearch.BadRequestError: BadRequestError(400, 'parsing_exception', 'Unknown key for a VALUE_STRING in [query].')\n",
    "    \n",
    "    candidates = es.search(index=es_index_name, query=exact_queries_col, size=query_size)['hits']['hits']\n",
    "    # CHANGELOG: To accomplish with the syntax of ES 8.x we need to change the line from:\n",
    "    # candidates = es.search(index=es_index_name, body=exact_queries_col)['hits']['hits']\n",
    "    # to \n",
    "    # candidates = es.search(index=es_index_name, query=exact_queries_col, size=query_size)['hits']['hits']\n",
    "    # This could avoid the following errors: \n",
    "    # # ValueError: Couldn't merge 'body' with other parameters as it wasn't a mapping. Instead of using 'body' use individual API parameters\n",
    "    # # elasticsearch.BadRequestError: BadRequestError(400, 'parsing_exception', 'Unknown key for a VALUE_STRING in [query].')\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        best_score_id, best_score_value, scores = 'null', 'null', 'null'\n",
    "        return T.Row('best_candidate_exact', 'sim_best_candidate_exact', 'similarity_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "    else:\n",
    "        cols_and_values = get_match_cols_and_values(vars_col, 'general', True)\n",
    "        best_score_id, best_score_value, scores = find_best_candidates(cols_and_values, candidates)\n",
    "\n",
    "        if float(best_score_value) >= float(config_['cutoff_exact_match']):\n",
    "            return T.Row('best_candidate_exact', 'sim_best_candidate_exact', 'similarity_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "        else: \n",
    "            best_score_id, best_score_value, scores = 'null', 'null', 'null'\n",
    "            return T.Row('best_candidate_exact', 'sim_best_candidate_exact', 'similarity_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "\n",
    "schema = StructType([StructField(\"best_candidate_exact\", StringType(), False), \n",
    "                     StructField(\"sim_best_candidate_exact\", StringType(), False), \n",
    "                     StructField(\"similarity_exact_candidates\", StringType(), False)])\n",
    "udf_find_elasticsearch_exact_best_candidate = F.udf(find_elasticsearch_exact_best_candidate, schema)\n",
    "\n",
    "\n",
    "def find_elasticsearch_non_exact_best_candidate(vars_col, non_exact_queries_col):\n",
    "    \"\"\"\n",
    "    Let us suppose a column with the following query:\n",
    "    \n",
    "    '{ \"bool\": { \n",
    "                 \"should\": [ \n",
    "                     {\"match\": {\"nome_a\":\"ROBESPIERRE PITA\", \"fuzziness\":\"AUTO\", \"operator\":\"or\", \"boost\":\"3.0\"}},\n",
    "                     {\"match\": {\"birthdate\":\"19870505\"}} ] } }'\n",
    "    \n",
    "    so, this function must return a dict with N results like: \n",
    "        {'_index': 'test', '_type': '_doc', '_id': 'aaabbbccc', '_score': 43.9280841,\n",
    "        '_source': {'name': 'ROBESPIERRE PITA', 'birthdate': '19870505', 'other_col': 'other_value'}},\n",
    "    \n",
    "    being N the query_size value set on config, you can see this number on the 'size' field of the query.\n",
    "    \n",
    "    This result can now be used to compute the proper similarity and pick the \n",
    "    best candidate for each record\n",
    "    \"\"\"\n",
    "    from elasticsearch import Elasticsearch\n",
    "    from ast import literal_eval\n",
    "    \n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    es_connect_string = config_['es_connect_string']\n",
    "    es_index_name = config_['es_index_name']\n",
    "    query_size = config_['query_size']\n",
    "    \n",
    "    es = Elasticsearch(es_connect_string)\n",
    "    \n",
    "    non_exact_queries_col = literal_eval(non_exact_queries_col)\n",
    "    # CHANGELOG: To accomplish with the syntax of ES 8.x we need to add the lines\n",
    "    # from ast import literal_eval\n",
    "    # exact_queries_col = literal_eval(exact_queries_col)\n",
    "    # String typed query contents does not meet the new requisits, rainsing the following error (even when they are properly written): \n",
    "    # # elasticsearch.BadRequestError: BadRequestError(400, 'parsing_exception', 'Unknown key for a VALUE_STRING in [query].')\n",
    "    \n",
    "    \n",
    "    candidates = es.search(index=es_index_name, query=non_exact_queries_col, size=query_size)['hits']['hits']\n",
    "    # CHANGELOG: To accomplish with the syntax of ES 8.x we need to change the line from:\n",
    "    # candidates = es.search(index=es_index_name, body=non_exact_queries_col)['hits']['hits']\n",
    "    # to \n",
    "    # candidates = es.search(index=es_index_name, query=non_exact_queries_col, size=query_size)['hits']['hits']\n",
    "    # This could avoid the following errors: \n",
    "    # # ValueError: Couldn't merge 'body' with other parameters as it wasn't a mapping. Instead of using 'body' use individual API parameters\n",
    "    # # elasticsearch.BadRequestError: BadRequestError(400, 'parsing_exception', 'Unknown key for a VALUE_STRING in [query].')\n",
    "    \n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        best_score_id, best_score_value, scores = 'null', 'null', 'null'\n",
    "        return T.Row('best_candidate_non_exact', 'sim_best_candidate_non_exact', 'similarity_non_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "    else:\n",
    "        cols_and_values = get_match_cols_and_values(vars_col, 'general', True)\n",
    "        best_score_id, best_score_value, scores = find_best_candidates(cols_and_values, candidates)\n",
    "        return T.Row('best_candidate_non_exact', 'sim_best_candidate_non_exact', 'similarity_non_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "        \n",
    "schema = StructType([StructField(\"best_candidate_non_exact\", StringType(), False), \n",
    "                     StructField(\"sim_best_candidate_non_exact\", StringType(), False), \n",
    "                     StructField(\"similarity_non_exact_candidates\", StringType(), False)])\n",
    "udf_find_elasticsearch_non_exact_best_candidate = F.udf(find_elasticsearch_non_exact_best_candidate, schema)\n",
    "\n",
    "\n",
    "\n",
    "def find_best_candidates(cols_and_values, candidates):\n",
    "    \n",
    "    config_ = config_bc.value\n",
    "    indexed_id_col = config_['datasets_info']['indexed_dataset']['id_column_name']\n",
    "    id_value = cols_and_values[indexed_id_col]\n",
    "    scores = {}\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        candidate_id = candidate['_source'][indexed_id_col]\n",
    "        sim_candidate = []\n",
    "\n",
    "        for col_and_value in list(cols_and_values.keys()):\n",
    "            if col_and_value != indexed_id_col:\n",
    "                comparison_info = [config_['comparisons'][x] for x in config_['comparisons'] if config_['comparisons'][x]['indexed_col'] == col_and_value][0]\n",
    "                n_comparisons = len(config_['comparisons'].keys())\n",
    "\n",
    "                sim_for_pair_of_cols = similarity_hub(n_comparisons, comparison_info, cols_and_values[col_and_value], candidate['_source'][col_and_value])\n",
    "\n",
    "                sim_candidate.append(sim_for_pair_of_cols)\n",
    "\n",
    "        score_max = sum([float(config_['comparisons'][x]['weight']) for x in config_['comparisons']])\n",
    "        score = (sum(sim_candidate))/score_max\n",
    "    \n",
    "        scores[candidate_id] = score\n",
    "    \n",
    "    if len(scores) > 0:\n",
    "        best_score_id = max(scores, key=scores.get)\n",
    "        best_score_value = scores[best_score_id]\n",
    "    else: \n",
    "        best_score_id = 'null'\n",
    "        best_score_value = '0.0'\n",
    "        scores = '{}'\n",
    "    return best_score_id, best_score_value, scores\n",
    "    \n",
    "    \n",
    "def similarity_hub(n_comparisons, comparison_info, col_and_value, candidate):\n",
    "    \"\"\"\n",
    "    Currently the CIDACS-RL uses overlap for categorical data, jaro_winkler for names and hamming for dates.\n",
    "    \"\"\"\n",
    "    import jellyfish\n",
    "    \n",
    "    # getting relevant information for this pair of values\n",
    "    config_ = config_bc.value\n",
    "#     score_max = sum([float(config_['comparisons'][x]['weight']) for x in config_['comparisons']])\n",
    "    similarity = 0.0\n",
    "    weight = float(comparison_info['weight'])\n",
    "    penalty = float(comparison_info['penalty'])\n",
    "    \n",
    "    # first, test if some value are missing\n",
    "    if (candidate == config_['null_value']) or (col_and_value == config_['null_value'])\\\n",
    "        or (candidate == \"\") or (col_and_value == \"\") or (candidate == None) or (col_and_value == None):\n",
    "        similarity = similarity - penalty\n",
    "    else: \n",
    "        sim_type = comparison_info['similarity']\n",
    "        if (sim_type == 'overlap') and (col_and_value == candidate):\n",
    "            similarity += (1.0) * weight\n",
    "            return similarity\n",
    "        elif (sim_type == 'overlap') and (col_and_value != candidate):\n",
    "            similarity += 0.0\n",
    "            return similarity\n",
    "        elif sim_type == 'jaro_winkler':\n",
    "            similarity += jellyfish.jaro_winkler(col_and_value, candidate) * weight\n",
    "        elif sim_type == 'hamming':\n",
    "            max_size = max(len(col_and_value), len(candidate))\n",
    "            similarity += 1.0 - float(jellyfish.hamming_distance(col_and_value, candidate)/max_size) * weight\n",
    "        else: \n",
    "            print('Please inform valid similarities for cidacs-rl')\n",
    "        \n",
    "        similarity = similarity\n",
    "    return similarity    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGORA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cidacs_rl_exact_phase(tolink_dataset):\n",
    "    \"\"\"\n",
    "    This function take a dataframe to link with an indexed dataframe on elasticsearch.\n",
    "    It consists in three main steps: \n",
    "        1) The first step consists in create an array column from a set of columns used on integration\n",
    "        \n",
    "        withColumn('vars', F.array(tolink_cols)) input: \n",
    "        +-----------+--------------------+------+\n",
    "        |id_cidacs_b|                nome|  sexo|\n",
    "        +-----------+--------------------+------+\n",
    "        |          0|    ROBESPIERRE PITA|     1|\n",
    "        +-----------+--------------------+------+\n",
    "        \n",
    "        withColumn('vars', F.array(tolink_cols)) output: \n",
    "        +-----------+--------------------+------+--------------------------+\n",
    "        |id_cidacs_b|                nome|  sexo|                      vars|\n",
    "        +-----------+--------------------+------+--------------------------+\n",
    "        |        614|    ROBESPIERRE PITA|     1|  [0, ROBESPIERRE PITA, 1]|\n",
    "        +-----------+--------------------+------+--------------------------+\n",
    "        \n",
    "        2) The second step will take the new array col as input and build exact queries:\n",
    "        \n",
    "        udf_build_exact_queries(F.col('vars')) output:\n",
    "        \n",
    "        { \"size\": \"50\",\n",
    "            \"query\": { \"bool\": \n",
    "            { \"must\": [ \n",
    "                {\"match\": {\"nome_a\":\"ROBESPIERRE PITA\"}},\n",
    "                {\"match\": {\"sexo_a\":\"1\"}} ] } } }\n",
    "        \n",
    "        +-----------+-----------------+------+--------------------------+----------------+\n",
    "        |id_cidacs_b|             nome|  sexo|                      vars|     exact_query|\n",
    "        +-----------+-----------------+------+--------------------------+----------------+\n",
    "        |        614|    ROBESPIERR...|     1|  [0, ROBESPIERRE PITA, 1]| { \"size\": \"5...|\n",
    "        +-----------+-----------------+------+--------------------------+----------------+\n",
    "        \n",
    "        3) Finally, a udf should generate 3 new columns with the best candidate id, the similarity with \n",
    "           this best candidate, and the set of candidates scores. \n",
    "        \n",
    "        +--------------------+------------------------+---------------------------+\n",
    "        |best_candidate_exact|sim_best_candidate_exact|similarity_exact_candidates|\n",
    "        +--------------------+------------------------+---------------------------+\n",
    "        |                 614|                       1|        {614: 1, 34: 0.8...|\n",
    "        +--------------------+------------------------+---------------------------+\n",
    "        \n",
    "    At last, this function should return the tolink_dataset with all these columns\n",
    "    \"\"\" \n",
    "    start = time.time()\n",
    "    # ------------------------------------ #\n",
    "    # getting relevant values from config\n",
    "    # ------------------------------------ #\n",
    "    \n",
    "    # collecting config json from broadcasted variable\n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    tolink_id_column = config_['datasets_info']['tolink_dataset']['id_column_name']\n",
    "    \n",
    "    tolink_columns = config_['datasets_info']['tolink_dataset']['columns']\n",
    "    \n",
    "    temp_dir = config_['temp_dir']\n",
    "    \n",
    "    paralelism = int(config_['datasets_info']['indexed_dataset']['default_paralelism'])\n",
    "\n",
    "    prefix_sl = \"StorageLevel.\"\n",
    "    storage_level = config_['datasets_info']['indexed_dataset']['storage_level']\n",
    "    \n",
    "    write_checkpoint = config_['write_checkpoint']\n",
    "    \n",
    "    # ------------------------------------ #\n",
    "    # preparing exact search\n",
    "    # ------------------------------------ #\n",
    "    # selecting columns\n",
    "    tolink_dataset = tolink_dataset.select(tolink_columns)\n",
    "    # building array of variable values\n",
    "    tolink_dataset = tolink_dataset.withColumn('vars', F.array(tolink_columns))\n",
    "    # building exact queries\n",
    "    tolink_dataset = tolink_dataset.withColumn('exact_queries', udf_build_exact_queries(F.col('vars')))\n",
    "    # finding the best candidate for each tolink record\n",
    "    tolink_dataset = tolink_dataset.withColumn('result_exact_search', F.explode(F.array(udf_find_elasticsearch_exact_best_candidate(F.col('vars'), F.col('exact_queries')))))\n",
    "    \n",
    "    if write_checkpoint == 'true':\n",
    "        # writing temporary data from this point helps to reset the DAG and improve performance\n",
    "        tolink_dataset.write.parquet(temp_dir+'result_exact_search.parquet', mode='overwrite')\n",
    "        tolink_dataset = spark.read.parquet(temp_dir+'result_exact_search.parquet').repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "    \n",
    "    # exploding array columns from the last function into 4 atomic cols\n",
    "    tolink_dataset = tolink_dataset.withColumn('best_candidate_exact', tolink_dataset.result_exact_search['best_candidate_exact'])\n",
    "    tolink_dataset = tolink_dataset.withColumn('sim_best_candidate_exact', tolink_dataset.result_exact_search['sim_best_candidate_exact'])\n",
    "    tolink_dataset = tolink_dataset.withColumn('similarity_exact_candidates', tolink_dataset.result_exact_search['similarity_exact_candidates'])\n",
    "    \n",
    "    tolink_dataset = tolink_dataset.withColumn('sim_best_candidate_exact', F.col('sim_best_candidate_exact').cast('float'))\n",
    "    \n",
    "    # dropping array columns\n",
    "    cols_to_drop = ['result_exact_search']\n",
    "    tolink_dataset = tolink_dataset.drop(*cols_to_drop)\n",
    "    \n",
    "    print(\"\\t[CIDACS-RL] time for exact phase: {} secs\".format(time.time()-start))\n",
    "    return tolink_dataset\n",
    "\n",
    "\n",
    "\n",
    "def cidacs_rl_non_exact_phase(tolink_dataset):\n",
    "    \"\"\"\n",
    "    This function take a dataframe from exact match phase and submit it to a non exact search.\n",
    "    cidacs_rl_non_exact_phase(tolink_dataset) input: \n",
    "    \n",
    "    +--------------------------+--------------------+------------------------+---------------------------+\n",
    "    |                      vars|best_candidate_exact|sim_best_candidate_exact|similarity_exact_candidates|\n",
    "    +--------------------------+--------------------+------------------------+---------------------------+\n",
    "    |       [2, SAMILA SENA, 2]|                null|                    null|                       null|\n",
    "    +--------------------------+--------------------+------------------------+---------------------------+\n",
    "        \n",
    "    cidacs_rl_non_exact_phase(tolink_dataset) output: \n",
    "        \n",
    "        +------------------------+----------------------------+-------------------------------+\n",
    "        |best_candidate_non_exact|sim_best_candidate_non_exact|similarity_exact_non_candidates|\n",
    "        +------------------------+----------------------------+-------------------------------+\n",
    "        |                       7|                        0.94|            {7: 0.94, 3: 0.9...|\n",
    "        +------------------------+----------------------------+-------------------------------+\n",
    "    \n",
    "    At last, this function should return the tolink_dataset with all these columns\n",
    "    \"\"\"\n",
    "    # ------------------------------------ #\n",
    "    # getting relevant values from config\n",
    "    # ------------------------------------ #\n",
    "    \n",
    "    # collecting config json from broadcasted variable\n",
    "    start = time.time()\n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    tolink_id_column = config_['datasets_info']['tolink_dataset']['id_column_name']\n",
    "    \n",
    "    tolink_columns = config_['datasets_info']['tolink_dataset']['columns']\n",
    "    \n",
    "    temp_dir = config_['temp_dir']\n",
    "    \n",
    "    is_debug = config_['debug']\n",
    "    \n",
    "    paralelism = int(config_['datasets_info']['indexed_dataset']['default_paralelism'])\n",
    "\n",
    "    prefix_sl = \"StorageLevel.\"\n",
    "    storage_level = config_['datasets_info']['indexed_dataset']['storage_level']\n",
    "    \n",
    "    write_checkpoint = config_['write_checkpoint']\n",
    "    # ------------------------------------ #\n",
    "    # preparing non exact search\n",
    "    # ------------------------------------ #\n",
    "    \n",
    "    # building linked_from column. Non-null values on sim_best_candidate_exact must be filled \n",
    "    # as 'exact_match', otherwise as 'non_exact_match'.    \n",
    "    filter_isnull = F.col('sim_best_candidate_exact').isNull()\n",
    "    tolink_dataset = tolink_dataset.withColumn('linked_from', F.when(filter_isnull, 'non_exact_match').otherwise('exact_match'))\n",
    "    \n",
    "    # preparing filters for debug and non-debug executions\n",
    "    filter_exact = F.col('linked_from') == 'exact_match'\n",
    "    filter_non_exact = F.col('linked_from') == 'non_exact_match'\n",
    "    \n",
    "    if is_debug == 'false': \n",
    "        # declaring a filtered version of input dataset\n",
    "        tolink_dataset_ = tolink_dataset.filter(filter_non_exact)\n",
    "        # declaring the remainder dataframe\n",
    "        tolink_dataset = tolink_dataset.filter(filter_exact)\n",
    "        \n",
    "        # creating, for remainder dataframe, the cols created in this function to ensure union\n",
    "        tolink_dataset = tolink_dataset.withColumn('best_candidate_non_exact', F.lit(None))\n",
    "        tolink_dataset = tolink_dataset.withColumn('sim_best_candidate_non_exact', F.lit(None))\n",
    "        tolink_dataset = tolink_dataset.withColumn('similarity_non_exact_candidates', F.lit(None))\n",
    "        tolink_dataset = tolink_dataset.withColumn('non_exact_queries', F.lit(None))\n",
    "    else: \n",
    "        # inside dataframe receives the input integrally\n",
    "        tolink_dataset_ = tolink_dataset\n",
    "    \n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('non_exact_queries', udf_build_non_exact_queries(F.col('vars')))\n",
    "\n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('result_non_exact_search', F.explode(F.array(udf_find_elasticsearch_non_exact_best_candidate(F.col('vars'), F.col('non_exact_queries')))))\n",
    "    \n",
    "    if write_checkpoint == 'true':\n",
    "        # writing temporary data from this point helps to reset the DAG and improve performance\n",
    "        tolink_dataset_.write.parquet(temp_dir+'result_non_exact_search.parquet', mode='overwrite')\n",
    "        tolink_dataset_ = spark.read.parquet(temp_dir+'result_non_exact_search.parquet').repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "\n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('best_candidate_non_exact', tolink_dataset_.result_non_exact_search['best_candidate_non_exact'])\n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('sim_best_candidate_non_exact', tolink_dataset_.result_non_exact_search['sim_best_candidate_non_exact'])\n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('similarity_non_exact_candidates', tolink_dataset_.result_non_exact_search['similarity_non_exact_candidates'])\n",
    "    \n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('sim_best_candidate_non_exact', F.col('sim_best_candidate_non_exact').cast('float'))\n",
    "    \n",
    "    cols_to_drop = ['result_non_exact_search']\n",
    "    tolink_dataset_ = tolink_dataset_.drop(*cols_to_drop)\n",
    "    \n",
    "    if is_debug == 'false':\n",
    "        tolink_dataset_ = tolink_dataset_.union(tolink_dataset)\n",
    "    print(\"\\t[CIDACS-RL] time for non-exact phase: {} secs\".format(time.time()-start))\n",
    "    return tolink_dataset_\n",
    "\n",
    "\n",
    "\n",
    "def cidacsrl():\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"[CIDACS-RL] starting at {}\".format(dt_string))\n",
    "    start = time.time()\n",
    "\n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    # getting the auxiliary variables\n",
    "    data_ext = config_['datasets_info']['indexed_dataset']['extension']\n",
    "    data_path = config_['datasets_info']['indexed_dataset']['path']\n",
    "    index_df_response = config_['index_data']\n",
    "    index_name = config_['es_index_name']\n",
    "\n",
    "    if index_df_response == 'yes':\n",
    "        start_ = time.time()\n",
    "        # getting the auxiliary variables\n",
    "        data_ext = config_['datasets_info']['indexed_dataset']['extension']\n",
    "        data_path = config_['datasets_info']['indexed_dataset']['path']\n",
    "        \n",
    "        paralelism = int(config_['datasets_info']['indexed_dataset']['default_paralelism'])\n",
    "        \n",
    "        prefix_sl = \"StorageLevel.\"\n",
    "        storage_level = config_['datasets_info']['indexed_dataset']['storage_level']\n",
    "        \n",
    "        # test the extension of the dataset to properly read it\n",
    "        if data_ext == 'csv':\n",
    "            indexed_dataset = spark.read.csv(data_path, header=True).repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "        elif data_ext == 'parquet':\n",
    "            indexed_dataset = spark.read.parquet(data_path).repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "        else:\n",
    "            print(\"Please make sure the extension for this dataset is set as 'csv' or 'parquet'\")\n",
    "    \n",
    "        # # indexing, at last\n",
    "        index_dataframe(indexed_dataset, index_name)\n",
    "        print(\"[CIDACS-RL] indexing on, it took {} secs\".format(time.time()-start_))\n",
    "    \n",
    "    # getting the auxiliary variables\n",
    "    data_ext = config_['datasets_info']['tolink_dataset']['extension']\n",
    "    data_path = config_['datasets_info']['tolink_dataset']['path']\n",
    "    \n",
    "    paralelism = int(config_['datasets_info']['tolink_dataset']['default_paralelism'])\n",
    "    \n",
    "    prefix_sl = \"StorageLevel.\"\n",
    "    storage_level = config_['datasets_info']['tolink_dataset']['storage_level']\n",
    "\n",
    "    # test the extension of the dataset to properly read it\n",
    "    if data_ext == 'csv':\n",
    "        tolink_dataset = spark.read.csv(data_path, header=True).repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "    elif data_ext == 'parquet':\n",
    "        tolink_dataset = spark.read.parquet(data_path).repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "    else:\n",
    "        print(\"Please make sure the extension for this dataset is set as 'csv' or 'parquet'\")\n",
    "    \n",
    "    tolink_dataset = cidacs_rl_exact_phase(tolink_dataset)\n",
    "    \n",
    "    tolink_dataset = cidacs_rl_non_exact_phase(tolink_dataset)\n",
    "    \n",
    "    tolink_dataset = tolink_dataset.withColumn('final_cidacs_rl_score', \n",
    "                                               F.when(F.col('linked_from') == 'exact_match', F.col('sim_best_candidate_exact'))\\\n",
    "                                                .otherwise(F.col('sim_best_candidate_non_exact')))\n",
    "    \n",
    "    tolink_dataset = tolink_dataset.withColumn('final_cidacs_rl_id', \n",
    "                                               F.when(F.col('linked_from') == 'exact_match', F.col('best_candidate_exact'))\\\n",
    "                                                .otherwise(F.col('best_candidate_non_exact')))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"[CIDACS-RL] finished at {}\".format(dt_string))\n",
    "    print(\"[CIDACS-RL] total time elapsed: {} secs\".format(time.time()-start))\n",
    "    return tolink_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running CIDACS-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'config.json'\n",
    "f = open(config_file)\n",
    "config = json.load(f)\n",
    "\n",
    "# broadcasting config\n",
    "config_bc = sc.broadcast(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_data': 'no',\n",
       " 'es_index_name': 'fd-cidacs-rl',\n",
       " 'es_connect_string': 'http://localhost:9200',\n",
       " 'query_size': 50,\n",
       " 'cutoff_exact_match': '0.95',\n",
       " 'null_value': '99',\n",
       " 'temp_dir': 'hdfs://barravento:9000/data/temp_dataframe/',\n",
       " 'debug': 'false',\n",
       " 'write_checkpoint': 'false',\n",
       " 'datasets_info': {'indexed_dataset': {'path': 'hdfs://barravento:9000/data/synthetic-dataset-A.parquet',\n",
       "   'extension': 'parquet',\n",
       "   'columns': ['id_cidacs_a', 'nome_a', 'nome_mae_a', 'dt_nasc_a', 'sexo_a'],\n",
       "   'id_column_name': 'id_cidacs_a',\n",
       "   'storage_level': 'MEMORY_ONLY',\n",
       "   'default_paralelism': '16'},\n",
       "  'tolink_dataset': {'path': 'hdfs://barravento:9000/data/synthetic-datasets-b-1000.parquet',\n",
       "   'extension': 'parquet',\n",
       "   'columns': ['id_cidacs_b', 'nome_b', 'nome_mae_b', 'dt_nasc_b', 'sexo_b'],\n",
       "   'id_column_name': 'id_cidacs_b',\n",
       "   'storage_level': 'MEMORY_ONLY',\n",
       "   'default_paralelism': '16'},\n",
       "  'result_dataset': {'path': 'hdfs://barravento:9000/data/result/'}},\n",
       " 'comparisons': {'name': {'indexed_col': 'nome_a',\n",
       "   'tolink_col': 'nome_b',\n",
       "   'must_match': 'true',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'true',\n",
       "   'boost': '3.0',\n",
       "   'query_type': 'match',\n",
       "   'similarity': 'jaro_winkler',\n",
       "   'weight': 5.0,\n",
       "   'penalty': 0.02},\n",
       "  'mothers_name': {'indexed_col': 'nome_mae_a',\n",
       "   'tolink_col': 'nome_mae_b',\n",
       "   'must_match': 'true',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'true',\n",
       "   'boost': '2.0',\n",
       "   'query_type': 'match',\n",
       "   'similarity': 'jaro_winkler',\n",
       "   'weight': 5.0,\n",
       "   'penalty': 0.02},\n",
       "  'birthdate': {'indexed_col': 'dt_nasc_a',\n",
       "   'tolink_col': 'dt_nasc_b',\n",
       "   'must_match': 'false',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'false',\n",
       "   'boost': '',\n",
       "   'query_type': 'term',\n",
       "   'similarity': 'hamming',\n",
       "   'weight': 1.0,\n",
       "   'penalty': 0.02},\n",
       "  'sex': {'indexed_col': 'sexo_a',\n",
       "   'tolink_col': 'sexo_b',\n",
       "   'must_match': 'true',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'false',\n",
       "   'boost': '',\n",
       "   'query_type': 'term',\n",
       "   'similarity': 'overlap',\n",
       "   'weight': 3.0,\n",
       "   'penalty': 0.02}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIDACS-RL] starting at 13/08/2025 17:53:03\n",
      "\t[CIDACS-RL] time for exact phase: 0.455289363861084 secs\n",
      "\t[CIDACS-RL] time for non-exact phase: 0.2842109203338623 secs\n",
      "[CIDACS-RL] finished at 13/08/2025 17:53:07\n",
      "[CIDACS-RL] total time elapsed: 4.650001764297485 secs\n"
     ]
    }
   ],
   "source": [
    "linked_data = cidacsrl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cidacs_b</th>\n",
       "      <th>nome_b</th>\n",
       "      <th>nome_mae_b</th>\n",
       "      <th>dt_nasc_b</th>\n",
       "      <th>sexo_b</th>\n",
       "      <th>vars</th>\n",
       "      <th>exact_queries</th>\n",
       "      <th>best_candidate_exact</th>\n",
       "      <th>sim_best_candidate_exact</th>\n",
       "      <th>similarity_exact_candidates</th>\n",
       "      <th>linked_from</th>\n",
       "      <th>non_exact_queries</th>\n",
       "      <th>best_candidate_non_exact</th>\n",
       "      <th>sim_best_candidate_non_exact</th>\n",
       "      <th>similarity_non_exact_candidates</th>\n",
       "      <th>final_cidacs_rl_score</th>\n",
       "      <th>final_cidacs_rl_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>765117</td>\n",
       "      <td>TAIANE CONCEICAO</td>\n",
       "      <td>MICHELINE DOS</td>\n",
       "      <td>20110620</td>\n",
       "      <td>1</td>\n",
       "      <td>[765117, TAIANE CONCEICAO, MICHELINE DOS, 2011...</td>\n",
       "      <td>{ \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"TAI...</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>non_exact_match</td>\n",
       "      <td>{ \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...</td>\n",
       "      <td>803829</td>\n",
       "      <td>0.821319</td>\n",
       "      <td>{48945=0.5632423132423133, 542523=0.5869449994...</td>\n",
       "      <td>0.821319</td>\n",
       "      <td>803829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>399018</td>\n",
       "      <td>MATEUS HENRIQUE DINIZ DE</td>\n",
       "      <td>MARCIA GLORIA DOS</td>\n",
       "      <td>20090906</td>\n",
       "      <td>2</td>\n",
       "      <td>[399018, MATEUS HENRIQUE DINIZ DE, MARCIA GLOR...</td>\n",
       "      <td>{ \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"MAT...</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>non_exact_match</td>\n",
       "      <td>{ \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...</td>\n",
       "      <td>786138</td>\n",
       "      <td>0.839724</td>\n",
       "      <td>{670002=0.5863545418167266, 737748=0.621335200...</td>\n",
       "      <td>0.839724</td>\n",
       "      <td>786138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15409</td>\n",
       "      <td>MARCIA FONSECA DOS REIS OLIVEIRA</td>\n",
       "      <td>MARCIA FONSECA DOS REIS OLIVEIRA</td>\n",
       "      <td>20091117</td>\n",
       "      <td>2</td>\n",
       "      <td>[15409, MARCIA FONSECA DOS REIS OLIVEIRA, MARC...</td>\n",
       "      <td>{ \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"MAR...</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>non_exact_match</td>\n",
       "      <td>{ \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...</td>\n",
       "      <td>128935</td>\n",
       "      <td>0.895805</td>\n",
       "      <td>{946753=0.6119678932178932, 99614=0.7624503968...</td>\n",
       "      <td>0.895805</td>\n",
       "      <td>128935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_cidacs_b                            nome_b  \\\n",
       "0      765117                  TAIANE CONCEICAO   \n",
       "1      399018          MATEUS HENRIQUE DINIZ DE   \n",
       "2       15409  MARCIA FONSECA DOS REIS OLIVEIRA   \n",
       "\n",
       "                         nome_mae_b dt_nasc_b sexo_b  \\\n",
       "0                     MICHELINE DOS  20110620      1   \n",
       "1                 MARCIA GLORIA DOS  20090906      2   \n",
       "2  MARCIA FONSECA DOS REIS OLIVEIRA  20091117      2   \n",
       "\n",
       "                                                vars  \\\n",
       "0  [765117, TAIANE CONCEICAO, MICHELINE DOS, 2011...   \n",
       "1  [399018, MATEUS HENRIQUE DINIZ DE, MARCIA GLOR...   \n",
       "2  [15409, MARCIA FONSECA DOS REIS OLIVEIRA, MARC...   \n",
       "\n",
       "                                       exact_queries best_candidate_exact  \\\n",
       "0  { \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"TAI...                 null   \n",
       "1  { \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"MAT...                 null   \n",
       "2  { \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"MAR...                 null   \n",
       "\n",
       "   sim_best_candidate_exact similarity_exact_candidates      linked_from  \\\n",
       "0                       NaN                        null  non_exact_match   \n",
       "1                       NaN                        null  non_exact_match   \n",
       "2                       NaN                        null  non_exact_match   \n",
       "\n",
       "                                   non_exact_queries best_candidate_non_exact  \\\n",
       "0  { \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...                   803829   \n",
       "1  { \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...                   786138   \n",
       "2  { \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...                   128935   \n",
       "\n",
       "   sim_best_candidate_non_exact  \\\n",
       "0                      0.821319   \n",
       "1                      0.839724   \n",
       "2                      0.895805   \n",
       "\n",
       "                     similarity_non_exact_candidates  final_cidacs_rl_score  \\\n",
       "0  {48945=0.5632423132423133, 542523=0.5869449994...               0.821319   \n",
       "1  {670002=0.5863545418167266, 737748=0.621335200...               0.839724   \n",
       "2  {946753=0.6119678932178932, 99614=0.7624503968...               0.895805   \n",
       "\n",
       "  final_cidacs_rl_id  \n",
       "0             803829  \n",
       "1             786138  \n",
       "2             128935  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linked_data.or.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "config_ = config_bc.value\n",
    "\n",
    "output_str_prefix = config_['datasets_info']['result_dataset']['path']\n",
    "output_str_all = \"{}cidacsrl-result_{}.parquet\".format(output_str_prefix, dt_string)\n",
    "\n",
    "linked_data.write.parquet(output_str_all, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CIDACS-RL] starting at 26/01/2022 20:56:21\n",
    "# \t[CIDACS-RL] time for exact phase: 38.057870864868164 secs\n",
    "# \t[CIDACS-RL] time for non-exact phase: 45.42489194869995 secs\n",
    "# [CIDACS-RL] finished at 26/01/2022 20:57:49\n",
    "# [CIDACS-RL] total time elapsed: 88.37369227409363 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total de execuo: 74.9164023399353 secs\n"
     ]
    }
   ],
   "source": [
    "print(\"Tempo total de execuo: {} secs\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tempo total de execuo: 105.01147866249084 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|    linked_from|count|\n",
      "+---------------+-----+\n",
      "|    exact_match|  542|\n",
      "|non_exact_match|  458|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linked_data.select('linked_from').groupBy('linked_from').count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
