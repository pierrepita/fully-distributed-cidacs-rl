{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import jellyfish\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option(\"display.max_columns\", 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CIDACSRL\") \\\n",
    "    .master(\"spark://barravento:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.elasticsearch:elasticsearch-spark-30_2.12:8.1.3\") \\\n",
    "    .config(\"spark.es.nodes\", \"barravento\") \\\n",
    "    .config(\"spark.es.port\", \"9200\") \\\n",
    "    .config(\"spark.es.nodes.wan.only\", \"false\") \\\n",
    "    .config(\"spark.es.resource\", \"dbb2\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 16) \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"256m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "# just to ensure that \n",
    "sc.setCheckpointDir(\"hdfs://barravento:9000/spark-checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ES functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_match_cols_and_values(vars_col, query_type, add_id_col):\n",
    "    \"\"\"\n",
    "    query_type must be 'exact' for building exact queries or 'general' for any else query and comparison.\n",
    "    \"\"\"\n",
    "    config_ = config_bc.value\n",
    "    # getting names of indexed columns\n",
    "    indexed_id_column = config_['datasets_info']['indexed_dataset']['id_column_name']\n",
    "    \n",
    "    indexed_cols = config_['datasets_info']['indexed_dataset']['columns']\n",
    "    \n",
    "#     if query_type == 'general':\n",
    "#         indexed_cols = [x for x in indexed_cols if x != indexed_id_column]\n",
    "        \n",
    "    # notice that we are linking indexed keys with tolink values\n",
    "    # the keys will be used to set which field will be fetched on es\n",
    "    # the values will be used as search content\n",
    "    tolink_cols_dict = dict(zip(indexed_cols, vars_col))\n",
    "    \n",
    "    if add_id_col == False:\n",
    "        tolink_cols_dict.pop(indexed_id_column, None)\n",
    "    \n",
    "    if query_type == 'general':\n",
    "        return tolink_cols_dict\n",
    "    elif query_type == 'exact':\n",
    "        # finding which are the columns used on exact match step\n",
    "        indexed_exact_match_vars = [indexed_id_column] + [config_['comparisons'][x]['indexed_col'] for x in config_['comparisons'] if config_['comparisons'][x]['must_match'] == 'true']\n",
    "        non_exact_match_cols = list(set(indexed_cols) - set(indexed_exact_match_vars))\n",
    "        # deleting those columns of non-exact match\n",
    "        [tolink_cols_dict.pop(x, None) for x in non_exact_match_cols]\n",
    "        \n",
    "        return tolink_cols_dict\n",
    "    else: \n",
    "        print(\"Please use 'general' or 'exact' as query_type input\")\n",
    "        return None, None\n",
    "\n",
    "def _resolve_storage_level(storage_level, prefix_sl=None):\n",
    "    \"\"\"\n",
    "    Safely resolve a StorageLevel from either a string or a StorageLevel object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    storage_level : Union[str, StorageLevel]\n",
    "        Examples of valid strings: \"MEMORY_AND_DISK\", \"DISK_ONLY\", \"MEMORY_ONLY_SER\".\n",
    "        You can also pass an actual StorageLevel object (e.g., StorageLevel.MEMORY_AND_DISK).\n",
    "    prefix_sl : Optional[str]\n",
    "        Backward-compat helper if your legacy code builds strings like \"StorageLevel.MEMORY_AND_DISK\".\n",
    "        If provided, we will fallback to eval(prefix_sl + storage_level) in case getattr() fails.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    StorageLevel\n",
    "    \"\"\"\n",
    "    if isinstance(storage_level, StorageLevel):\n",
    "        return storage_level\n",
    "    if isinstance(storage_level, str):\n",
    "        # First tries getattr on the StorageLevel class\n",
    "        try:\n",
    "            return getattr(StorageLevel, storage_level)\n",
    "        except AttributeError:\n",
    "            # Fallback for legacy patterns like prefix_sl=\"StorageLevel.\" and storage_level=\"MEMORY_AND_DISK\"\n",
    "            if prefix_sl:\n",
    "                return eval(prefix_sl + storage_level)  # last resort for legacy code\n",
    "            raise ValueError('Unknown StorageLevel string: {}'.format(storage_level))\n",
    "    # Default fallback\n",
    "    return StorageLevel.MEMORY_AND_DISK\n",
    "\n",
    "\n",
    "def optimize_with_checkpoint(df,\n",
    "                             parallelism,\n",
    "                             storage_level='MEMORY_AND_DISK',\n",
    "                             checkpoint_mode='durable',\n",
    "                             eager=True,\n",
    "                             materialize=True,\n",
    "                             materialize_action='count',\n",
    "                             prefix_sl=None,\n",
    "                             unpersist_prev=True):\n",
    "    \"\"\"\n",
    "    Repartition -> Persist -> (Optional) Materialize -> (Local)Checkpoint -> Unpersist previous DF.\n",
    "\n",
    "    WHY:\n",
    "      - Long Spark pipelines accumulate lineage; actions later can trigger a huge DAG,\n",
    "        many tasks, and repeated work. Writing/reading Parquet to \"reset\" the DAG works,\n",
    "        but it's expensive (I/O, compression, metastore).\n",
    "      - Using checkpoint() or localCheckpoint() *truncates* the lineage without heavy external I/O\n",
    "        (localCheckpoint is the fastest, but less fault-tolerant).\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        The DataFrame you want to stabilize.\n",
    "    parallelism : int\n",
    "        Number of partitions after the repartition() step.\n",
    "        Example: number of executors * cores per executor, adjusted by data size/skew.\n",
    "    storage_level : Union[str, StorageLevel], default \"MEMORY_AND_DISK\"\n",
    "        How you want to persist the DataFrame before checkpointing.\n",
    "        Examples: \"MEMORY_AND_DISK\", \"DISK_ONLY\", \"MEMORY_ONLY_SER\".\n",
    "    checkpoint_mode : {\"durable\", \"local\"}, default \"durable\"\n",
    "        \"durable\" -> df.checkpoint(eager=...)\n",
    "                    uses the configured checkpoint dir (e.g., HDFS) and is fault-tolerant.\n",
    "        \"local\"   -> df.localCheckpoint(eager=...)\n",
    "                    faster, avoids writing to the checkpoint dir, BUT less fault-tolerant.\n",
    "    eager : bool, default True\n",
    "        If True, checkpoint is executed eagerly (immediately). Recommended for predictability.\n",
    "    materialize : bool, default True\n",
    "        If True, forces an action after persist() (e.g., count()) to materialize cached data\n",
    "        *before* checkpointing. This helps avoid materialization during a later, more expensive stage.\n",
    "    materialize_action : {\"count\", \"take\"}, default \"count\"\n",
    "        Which light-weight action to use for materialization. \"count\" is predictable; \"take\" can be faster for huge datasets.\n",
    "    prefix_sl : Optional[str]\n",
    "        Legacy helper for resolving StorageLevel strings (see _resolve_storage_level docstring).\n",
    "    unpersist_prev : bool, default True\n",
    "        If True, attempts to unpersist the previous (pre-checkpoint) DataFrame to free executor memory.\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    pyspark.sql.DataFrame\n",
    "        A new DataFrame with truncated lineage (post-checkpoint).\n",
    "\n",
    "    EXAMPLE\n",
    "    -------\n",
    "        # 1) Ensure a durable checkpoint directory (once at session init):\n",
    "        # spark.sparkContext.setCheckpointDir(\"hdfs://barravento:9000/spark-checkpoints\")\n",
    "\n",
    "        # 2) Use in your pipeline:\n",
    "        df = optimize_with_checkpoint(\n",
    "                df,\n",
    "                parallelism=paralelism,\n",
    "                storage_level=\"MEMORY_AND_DISK\",\n",
    "                checkpoint_mode=\"durable\",   # or \"local\" for speed (less fault-tolerant)\n",
    "                eager=True,\n",
    "                materialize=True,\n",
    "                materialize_action=\"count\"\n",
    "             )\n",
    "    \"\"\"\n",
    "    sl = _resolve_storage_level(storage_level, prefix_sl)\n",
    "\n",
    "    # Keep reference to the original DF so we can unpersist it later.\n",
    "    prev_df = df\n",
    "\n",
    "    # (1) Repartition ONCE to balance load, then persist with the given StorageLevel.\n",
    "    df = df.repartition(parallelism).persist(sl)\n",
    "\n",
    "    # (2) Materialize the cache now with a cheap action.\n",
    "    if materialize:\n",
    "        try:\n",
    "            if materialize_action == 'take':\n",
    "                # Taking a small sample forces computation without scanning all rows.\n",
    "                _ = df.take(1)\n",
    "            else:\n",
    "                # Default and predictable: forces a single pass to populate cache.\n",
    "                _ = df.count()\n",
    "        except Exception:\n",
    "            # If the action fails, don't block the optimization flow; re-raise if you prefer strict behavior.\n",
    "            pass\n",
    "\n",
    "    # (3) Truncate lineage via (local)checkpoint.\n",
    "    if checkpoint_mode == 'local':\n",
    "        df = df.localCheckpoint(eager=eager)\n",
    "    else:\n",
    "        df = df.checkpoint(eager=eager)\n",
    "\n",
    "    # (4) Free memory from the previous cached DF (if any).\n",
    "    if unpersist_prev and prev_df is not None:\n",
    "        try:\n",
    "            prev_df.unpersist(blocking=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def index_dataframe(dataframe, es_index_name):\n",
    "    # creating new index\n",
    "    dataframe.write.format(\"org.elasticsearch.spark.sql\") \\\n",
    "                 .option(\"es.resource\", es_index_name).mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_exact_queries(vars_col): \n",
    "    \"\"\"\n",
    "    Let us suppose the following values:\n",
    "    vars_col = ['ROBESPIERRE PITA', '1987-05-05', '1', 'Mari Santos']\n",
    "    indexed_cols = ['name', 'birthdate', 'sex', 'mothers_name']\n",
    "    query_size = 10\n",
    "    \n",
    "    and only the first two attributes are assigned to exact match.\n",
    "    So, the resulting query column would be: \n",
    "    '{ \"size\": \"50\", \"query\": \n",
    "                    { \"bool\": { \"must\": [ \n",
    "                                {\"match\": {\"name\":\"ROBESPIERRE PITA\"}},\n",
    "                                {\"match\": {\"birthdate\":\"19870505\"}}] } } }'\n",
    "    Requirements: \n",
    "    - All values on vars_col must be converted into string\n",
    "    - All the hyphens symbols must be taken from date type used to search (e.g. 1987-05-05 must be converted to 19870505)\n",
    "    - The config json must be available as a broadcast through sc.broadcast() function.\n",
    "    - The names of indexed columns must be correctly filled. \n",
    "    \"\"\"\n",
    "    config_ = config_bc.value\n",
    "    query_size = config_['query_size']\n",
    "    \n",
    "    tolink_cols_dict = get_match_cols_and_values(vars_col, 'exact', False)\n",
    "    \n",
    "    # -------------------------------------------- #\n",
    "    #   starting the building of query string      #\n",
    "    # -------------------------------------------- #\n",
    "    # setting the preffix and suffix of query core\n",
    "    prefix_ = \"\"\"{\"match\": {\"\"\"\n",
    "    suffix_ = \"\"\"}}\"\"\"\n",
    "    \n",
    "    # filling the query core with all indexed columns and values from vars_col\n",
    "    strings = []\n",
    "    for col in list(tolink_cols_dict.keys()):\n",
    "        string = str(prefix_) + \"\\\"\" + str(col) + \"\\\"\" + \":\" + \"\\\"\" +  str(tolink_cols_dict[col]) + \"\\\"\" + str(suffix_)\n",
    "        strings.append(string)\n",
    "    \n",
    "    # building the query core. \n",
    "    # Should be like: {\"match\": {\"name\":\"ROBESPIERRE PITA\"}}, {\"birthdate\": {\"name\":\"1987-05-05\"}}\n",
    "    line = ','.join(strings)\n",
    "    \n",
    "    # Finally the final query string\n",
    "    complete_query = \"\"\"{ \"bool\": { \"must\": [ %s ] } }\"\"\" % (line)\n",
    "    # CHANGELOG: to accomplish with new syntax from ES 8.x, we need to change the complete query from\n",
    "    # complete_query = \"\"\"{ \"size\": \"%s\", \"query\": { \"bool\": { \"must\": [ %s ] } } }\"\"\" % (query_size,line)\n",
    "    # to\n",
    "    # complete_query = \"\"\"{ \"bool\": { \"must\": [ %s ] } } }\"\"\" % (line)\n",
    "    # read more in: https://github.com/elastic/elasticsearch-py/issues/1698\n",
    "    #               https://www.elastic.co/guide/en/elasticsearch/client/python-api/8.1/examples.html\n",
    "    \n",
    "    return complete_query\n",
    "udf_build_exact_queries = F.udf(build_exact_queries, StringType()) \n",
    "\n",
    "def build_non_exact_queries(vars_col): \n",
    "    \"\"\"\n",
    "    Let us suppose the following values:\n",
    "    vars_col = ['ROBESPIERRE PITA', '1987-05-05', '1', 'Mari Santos']\n",
    "    indexed_cols = ['name', 'birthdate', 'sex', 'mothers_name']\n",
    "    query_size = 10\n",
    "    \n",
    "    and only the first two attributes are assigned to exact match.\n",
    "    So, the resulting query column would be: \n",
    "    '{\"bool\": { \n",
    "                 \"should\": [ \n",
    "                     {'match': {'nome_a': {'query': 'ROBESPIERRE PITA', 'fuzziness':'AUTO', 'operator':'or', 'boost':'3.0'}}},\n",
    "                     {\"match\": {\"birthdate\":\"19870505\"}} ] } } }\n",
    "                     {\"term\": {\"sexo_a\":\"1\"}} ] } } }'\n",
    "    Requirements: \n",
    "    - All values on vars_col must be converted into string\n",
    "    - All the hyphens symbols must be taken from date type used to search (e.g. 1987-05-05 must be converted to 19870505)\n",
    "    - The config json must be available as a broadcast through sc.broadcast() function.\n",
    "    - The names of indexed columns must be correctly filled. \n",
    "    \"\"\"\n",
    "    config_ = config_bc.value\n",
    "    query_size = config_['query_size']\n",
    "    \n",
    "    tolink_cols_dict = get_match_cols_and_values(vars_col, 'exact', False)\n",
    "    \n",
    "    # -------------------------------------------- #\n",
    "    #   starting the building of query string      #\n",
    "    # -------------------------------------------- #\n",
    "    \n",
    "    # filling the query core with all indexed columns and values from vars_col\n",
    "    comparisons = [config_['comparisons'][x] for x in config_['comparisons']]\n",
    "    strings = []\n",
    "    for col in list(tolink_cols_dict.keys()):\n",
    "        query_col_instructions = [x for x in comparisons if x['indexed_col'] == col][0]\n",
    "        query_type = str(query_col_instructions['query_type'])\n",
    "        prefix_ = \"\"\"{\"%s\": {\"\"\" % query_type\n",
    "        suffix_ = \"\"\"}}\"\"\"\n",
    "\n",
    "        if query_col_instructions['should_match'] == 'true':\n",
    "            if query_col_instructions['is_fuzzy'] == 'true':\n",
    "                boost = str(query_col_instructions['boost'])\n",
    "                string = str(prefix_) + \"\\\"\" + str(col) + \"\\\"\" + \":\" + \" { \\\"query\\\" : \\\"\" +  str(tolink_cols_dict[col]) + \"\\\"\" + \", \\\"fuzziness\\\":\\\"AUTO\\\", \\\"operator\\\":\\\"or\\\", \\\"boost\\\":\\\"\" + boost + \"\\\" }\" + str(suffix_)\n",
    "                \n",
    "            if query_col_instructions['is_fuzzy'] == 'false':\n",
    "                string = str(prefix_) + \"\\\"\" + str(col) + \"\\\"\" + \":\" + \"\\\"\" +  str(tolink_cols_dict[col]) + \"\\\"\" + str(suffix_)\n",
    "        strings.append(string)\n",
    "    \n",
    "    # building the query core. \n",
    "    # is_fuzzy = 'true' should be like: {\"match\": {\"name\":\"ROBESPIERRE PITA\", \"fuzziness\":\"AUTO\", \"operator\":\"or\", \"boost\":\"3.0\"}}, {\"term\": {\"dt_nasc_a\":\"20070816\"}}\n",
    "    line = ','.join(strings)\n",
    "    \n",
    "    # Finally the final query string\n",
    "    complete_query = \"\"\"{ \"bool\": { \"should\": [ %s ] } }\"\"\" % (line)\n",
    "    # CHANGELOG: to accomplish with new syntax from ES 8.x, we need to change the complete query from\n",
    "    # complete_query = \"\"\"{ \"size\": \"%s\", \"query\": { \"bool\": { \"should\": [ %s ] } } }\"\"\" % (query_size,line)\n",
    "    # to\n",
    "    # complete_query = \"\"\"{ \"bool\": { \"should\": [ %s ] } } }\"\"\" % (query_size,line)\n",
    "    # read more in: https://github.com/elastic/elasticsearch-py/issues/1698\n",
    "    #               https://www.elastic.co/guide/en/elasticsearch/client/python-api/8.1/examples.html\n",
    "    \n",
    "    return complete_query\n",
    "udf_build_non_exact_queries = F.udf(build_non_exact_queries, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finding matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_elasticsearch_exact_best_candidate(vars_col, exact_queries_col):\n",
    "    \"\"\"\n",
    "    Let us suppose a column with the following query:\n",
    "    \n",
    "    '{ \"bool\": { \"must\": [ \n",
    "                    {\"match\": {\"name\":\"ROBESPIERRE PITA\"}},\n",
    "                    {\"match\": {\"birthdate\":\"19870505\"}}] } }'\n",
    "    \n",
    "    so, this function must return a dict with N results like: \n",
    "        {'_index': 'test', '_type': '_doc', '_id': 'aaabbbccc', '_score': 43.9280841,\n",
    "        '_source': {'name': 'ROBESPIERRE PITA', 'birthdate': '19870505', 'other_col': 'other_value'}},\n",
    "    \n",
    "    being N the query_size value set on config, you can see this number on the 'size' field of the query.\n",
    "    \n",
    "    This result can now be used to compute the proper similarity and pick the \n",
    "    best candidate for each record\n",
    "    \"\"\"\n",
    "    from elasticsearch import Elasticsearch\n",
    "    from ast import literal_eval\n",
    "\n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    es_connect_string = config_['es_connect_string']\n",
    "    es_index_name = config_['es_index_name']\n",
    "    query_size = config_['query_size']\n",
    "    \n",
    "    es = Elasticsearch(es_connect_string)\n",
    "    \n",
    "    exact_queries_col = literal_eval(exact_queries_col)\n",
    "    # CHANGELOG: To accomplish with the syntax of ES 8.x we need to add the lines\n",
    "    # from ast import literal_eval\n",
    "    # exact_queries_col = literal_eval(exact_queries_col)\n",
    "    # String typed query contents does not meet the new requisits, rainsing the following error (even when they are properly written): \n",
    "    # # elasticsearch.BadRequestError: BadRequestError(400, 'parsing_exception', 'Unknown key for a VALUE_STRING in [query].')\n",
    "    \n",
    "    candidates = es.search(index=es_index_name, query=exact_queries_col, size=query_size)['hits']['hits']\n",
    "    # CHANGELOG: To accomplish with the syntax of ES 8.x we need to change the line from:\n",
    "    # candidates = es.search(index=es_index_name, body=exact_queries_col)['hits']['hits']\n",
    "    # to \n",
    "    # candidates = es.search(index=es_index_name, query=exact_queries_col, size=query_size)['hits']['hits']\n",
    "    # This could avoid the following errors: \n",
    "    # # ValueError: Couldn't merge 'body' with other parameters as it wasn't a mapping. Instead of using 'body' use individual API parameters\n",
    "    # # elasticsearch.BadRequestError: BadRequestError(400, 'parsing_exception', 'Unknown key for a VALUE_STRING in [query].')\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        best_score_id, best_score_value, scores = 'null', 'null', 'null'\n",
    "        return T.Row('best_candidate_exact', 'sim_best_candidate_exact', 'similarity_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "    else:\n",
    "        cols_and_values = get_match_cols_and_values(vars_col, 'general', True)\n",
    "        best_score_id, best_score_value, scores = find_best_candidates(cols_and_values, candidates)\n",
    "\n",
    "        if float(best_score_value) >= float(config_['cutoff_exact_match']):\n",
    "            return T.Row('best_candidate_exact', 'sim_best_candidate_exact', 'similarity_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "        else: \n",
    "            best_score_id, best_score_value, scores = 'null', 'null', 'null'\n",
    "            return T.Row('best_candidate_exact', 'sim_best_candidate_exact', 'similarity_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "\n",
    "schema = StructType([StructField(\"best_candidate_exact\", StringType(), False), \n",
    "                     StructField(\"sim_best_candidate_exact\", StringType(), False), \n",
    "                     StructField(\"similarity_exact_candidates\", StringType(), False)])\n",
    "udf_find_elasticsearch_exact_best_candidate = F.udf(find_elasticsearch_exact_best_candidate, schema)\n",
    "\n",
    "\n",
    "def find_elasticsearch_non_exact_best_candidate(vars_col, non_exact_queries_col):\n",
    "    \"\"\"\n",
    "    Let us suppose a column with the following query:\n",
    "    \n",
    "    '{ \"bool\": { \n",
    "                 \"should\": [ \n",
    "                     {\"match\": {\"nome_a\":\"ROBESPIERRE PITA\", \"fuzziness\":\"AUTO\", \"operator\":\"or\", \"boost\":\"3.0\"}},\n",
    "                     {\"match\": {\"birthdate\":\"19870505\"}} ] } }'\n",
    "    \n",
    "    so, this function must return a dict with N results like: \n",
    "        {'_index': 'test', '_type': '_doc', '_id': 'aaabbbccc', '_score': 43.9280841,\n",
    "        '_source': {'name': 'ROBESPIERRE PITA', 'birthdate': '19870505', 'other_col': 'other_value'}},\n",
    "    \n",
    "    being N the query_size value set on config, you can see this number on the 'size' field of the query.\n",
    "    \n",
    "    This result can now be used to compute the proper similarity and pick the \n",
    "    best candidate for each record\n",
    "    \"\"\"\n",
    "    from elasticsearch import Elasticsearch\n",
    "    from ast import literal_eval\n",
    "    \n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    es_connect_string = config_['es_connect_string']\n",
    "    es_index_name = config_['es_index_name']\n",
    "    query_size = config_['query_size']\n",
    "    \n",
    "    es = Elasticsearch(es_connect_string)\n",
    "    \n",
    "    non_exact_queries_col = literal_eval(non_exact_queries_col)\n",
    "    # CHANGELOG: To accomplish with the syntax of ES 8.x we need to add the lines\n",
    "    # from ast import literal_eval\n",
    "    # exact_queries_col = literal_eval(exact_queries_col)\n",
    "    # String typed query contents does not meet the new requisits, rainsing the following error (even when they are properly written): \n",
    "    # # elasticsearch.BadRequestError: BadRequestError(400, 'parsing_exception', 'Unknown key for a VALUE_STRING in [query].')\n",
    "    \n",
    "    \n",
    "    candidates = es.search(index=es_index_name, query=non_exact_queries_col, size=query_size)['hits']['hits']\n",
    "    # CHANGELOG: To accomplish with the syntax of ES 8.x we need to change the line from:\n",
    "    # candidates = es.search(index=es_index_name, body=non_exact_queries_col)['hits']['hits']\n",
    "    # to \n",
    "    # candidates = es.search(index=es_index_name, query=non_exact_queries_col, size=query_size)['hits']['hits']\n",
    "    # This could avoid the following errors: \n",
    "    # # ValueError: Couldn't merge 'body' with other parameters as it wasn't a mapping. Instead of using 'body' use individual API parameters\n",
    "    # # elasticsearch.BadRequestError: BadRequestError(400, 'parsing_exception', 'Unknown key for a VALUE_STRING in [query].')\n",
    "    \n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        best_score_id, best_score_value, scores = 'null', 'null', 'null'\n",
    "        return T.Row('best_candidate_non_exact', 'sim_best_candidate_non_exact', 'similarity_non_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "    else:\n",
    "        cols_and_values = get_match_cols_and_values(vars_col, 'general', True)\n",
    "        best_score_id, best_score_value, scores = find_best_candidates(cols_and_values, candidates)\n",
    "        return T.Row('best_candidate_non_exact', 'sim_best_candidate_non_exact', 'similarity_non_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "        \n",
    "schema = StructType([StructField(\"best_candidate_non_exact\", StringType(), False), \n",
    "                     StructField(\"sim_best_candidate_non_exact\", StringType(), False), \n",
    "                     StructField(\"similarity_non_exact_candidates\", StringType(), False)])\n",
    "udf_find_elasticsearch_non_exact_best_candidate = F.udf(find_elasticsearch_non_exact_best_candidate, schema)\n",
    "\n",
    "\n",
    "\n",
    "def find_best_candidates(cols_and_values, candidates):\n",
    "    \n",
    "    config_ = config_bc.value\n",
    "    indexed_id_col = config_['datasets_info']['indexed_dataset']['id_column_name']\n",
    "    id_value = cols_and_values[indexed_id_col]\n",
    "    scores = {}\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        candidate_id = candidate['_source'][indexed_id_col]\n",
    "        sim_candidate = []\n",
    "\n",
    "        for col_and_value in list(cols_and_values.keys()):\n",
    "            if col_and_value != indexed_id_col:\n",
    "                comparison_info = [config_['comparisons'][x] for x in config_['comparisons'] if config_['comparisons'][x]['indexed_col'] == col_and_value][0]\n",
    "                n_comparisons = len(config_['comparisons'].keys())\n",
    "\n",
    "                sim_for_pair_of_cols = similarity_hub(n_comparisons, comparison_info, cols_and_values[col_and_value], candidate['_source'][col_and_value])\n",
    "\n",
    "                sim_candidate.append(sim_for_pair_of_cols)\n",
    "\n",
    "        score_max = sum([float(config_['comparisons'][x]['weight']) for x in config_['comparisons']])\n",
    "        score = (sum(sim_candidate))/score_max\n",
    "    \n",
    "        scores[candidate_id] = score\n",
    "    \n",
    "    if len(scores) > 0:\n",
    "        best_score_id = max(scores, key=scores.get)\n",
    "        best_score_value = scores[best_score_id]\n",
    "    else: \n",
    "        best_score_id = 'null'\n",
    "        best_score_value = '0.0'\n",
    "        scores = '{}'\n",
    "    return best_score_id, best_score_value, scores\n",
    "    \n",
    "    \n",
    "def similarity_hub(n_comparisons, comparison_info, col_and_value, candidate):\n",
    "    \"\"\"\n",
    "    Currently the CIDACS-RL uses overlap for categorical data, jaro_winkler for names and hamming for dates.\n",
    "    \"\"\"\n",
    "    import jellyfish\n",
    "    \n",
    "    # getting relevant information for this pair of values\n",
    "    config_ = config_bc.value\n",
    "#     score_max = sum([float(config_['comparisons'][x]['weight']) for x in config_['comparisons']])\n",
    "    similarity = 0.0\n",
    "    weight = float(comparison_info['weight'])\n",
    "    penalty = float(comparison_info['penalty'])\n",
    "    \n",
    "    # first, test if some value are missing\n",
    "    if (candidate == config_['null_value']) or (col_and_value == config_['null_value'])\\\n",
    "        or (candidate == \"\") or (col_and_value == \"\") or (candidate == None) or (col_and_value == None):\n",
    "        similarity = similarity - penalty\n",
    "    else: \n",
    "        sim_type = comparison_info['similarity']\n",
    "        if (sim_type == 'overlap') and (col_and_value == candidate):\n",
    "            similarity += (1.0) * weight\n",
    "            return similarity\n",
    "        elif (sim_type == 'overlap') and (col_and_value != candidate):\n",
    "            similarity += 0.0\n",
    "            return similarity\n",
    "        elif sim_type == 'jaro_winkler':\n",
    "            similarity += jellyfish.jaro_winkler(col_and_value, candidate) * weight\n",
    "        elif sim_type == 'hamming':\n",
    "            max_size = max(len(col_and_value), len(candidate))\n",
    "            similarity += 1.0 - float(jellyfish.hamming_distance(col_and_value, candidate)/max_size) * weight\n",
    "        else: \n",
    "            print('Please inform valid similarities for cidacs-rl')\n",
    "        \n",
    "        similarity = similarity\n",
    "    return similarity    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jellyfish.jaro_winkler(\"Robespierre\", \"Robespierre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cidacs_rl_exact_phase(tolink_dataset):\n",
    "    \"\"\"\n",
    "    This function take a dataframe to link with an indexed dataframe on elasticsearch.\n",
    "    It consists in three main steps: \n",
    "        1) The first step consists in create an array column from a set of columns used on integration\n",
    "        \n",
    "        withColumn('vars', F.array(tolink_cols)) input: \n",
    "        +-----------+--------------------+------+\n",
    "        |id_cidacs_b|                nome|  sexo|\n",
    "        +-----------+--------------------+------+\n",
    "        |          0|    ROBESPIERRE PITA|     1|\n",
    "        +-----------+--------------------+------+\n",
    "        \n",
    "        withColumn('vars', F.array(tolink_cols)) output: \n",
    "        +-----------+--------------------+------+--------------------------+\n",
    "        |id_cidacs_b|                nome|  sexo|                      vars|\n",
    "        +-----------+--------------------+------+--------------------------+\n",
    "        |        614|    ROBESPIERRE PITA|     1|  [0, ROBESPIERRE PITA, 1]|\n",
    "        +-----------+--------------------+------+--------------------------+\n",
    "        \n",
    "        2) The second step will take the new array col as input and build exact queries:\n",
    "        \n",
    "        udf_build_exact_queries(F.col('vars')) output:\n",
    "        \n",
    "        { \"size\": \"50\",\n",
    "            \"query\": { \"bool\": \n",
    "            { \"must\": [ \n",
    "                {\"match\": {\"nome_a\":\"ROBESPIERRE PITA\"}},\n",
    "                {\"match\": {\"sexo_a\":\"1\"}} ] } } }\n",
    "        \n",
    "        +-----------+-----------------+------+--------------------------+----------------+\n",
    "        |id_cidacs_b|             nome|  sexo|                      vars|     exact_query|\n",
    "        +-----------+-----------------+------+--------------------------+----------------+\n",
    "        |        614|    ROBESPIERR...|     1|  [0, ROBESPIERRE PITA, 1]| { \"size\": \"5...|\n",
    "        +-----------+-----------------+------+--------------------------+----------------+\n",
    "        \n",
    "        3) Finally, a udf should generate 3 new columns with the best candidate id, the similarity with \n",
    "           this best candidate, and the set of candidates scores. \n",
    "        \n",
    "        +--------------------+------------------------+---------------------------+\n",
    "        |best_candidate_exact|sim_best_candidate_exact|similarity_exact_candidates|\n",
    "        +--------------------+------------------------+---------------------------+\n",
    "        |                 614|                       1|        {614: 1, 34: 0.8...|\n",
    "        +--------------------+------------------------+---------------------------+\n",
    "        \n",
    "    At last, this function should return the tolink_dataset with all these columns\n",
    "    \"\"\" \n",
    "    start = time.time()\n",
    "    # ------------------------------------ #\n",
    "    # getting relevant values from config\n",
    "    # ------------------------------------ #\n",
    "    \n",
    "    # collecting config json from broadcasted variable\n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    tolink_id_column = config_['datasets_info']['tolink_dataset']['id_column_name']\n",
    "    \n",
    "    tolink_columns = config_['datasets_info']['tolink_dataset']['columns']\n",
    "    \n",
    "    temp_dir = config_['temp_dir']\n",
    "    \n",
    "    paralelism = int(config_['datasets_info']['indexed_dataset']['default_paralelism'])\n",
    "\n",
    "    prefix_sl = \"StorageLevel.\"\n",
    "    storage_level = config_['datasets_info']['indexed_dataset']['storage_level']\n",
    "    \n",
    "    write_checkpoint = config_['write_checkpoint']\n",
    "    \n",
    "    # ------------------------------------ #\n",
    "    # preparing exact search\n",
    "    # ------------------------------------ #\n",
    "    # selecting columns\n",
    "    tolink_dataset = tolink_dataset.select(tolink_columns)\n",
    "    # building array of variable values\n",
    "    tolink_dataset = tolink_dataset.withColumn('vars', F.array(tolink_columns))\n",
    "    # building exact queries\n",
    "    tolink_dataset = tolink_dataset.withColumn('exact_queries', udf_build_exact_queries(F.col('vars')))\n",
    "    # finding the best candidate for each tolink record\n",
    "    tolink_dataset = tolink_dataset.withColumn('result_exact_search', F.explode(F.array(udf_find_elasticsearch_exact_best_candidate(F.col('vars'), F.col('exact_queries')))))\n",
    "    \n",
    "    if write_checkpoint == 'true':\n",
    "        tolink_dataset = optimize_with_checkpoint(\n",
    "            df=tolink_dataset,\n",
    "            parallelism=paralelism,\n",
    "            storage_level=storage_level,      # ex.: \"MEMORY_AND_DISK\" ou StorageLevel.MEMORY_AND_DISK\n",
    "            checkpoint_mode='durable',        # ou 'local' para mais velocidade\n",
    "            eager=True,\n",
    "            materialize=True,\n",
    "            materialize_action='count',\n",
    "            prefix_sl=prefix_sl,              # se você usa algo como \"StorageLevel.\"\n",
    "            unpersist_prev=True\n",
    "        )\n",
    "    \n",
    "    # exploding array columns from the last function into 4 atomic cols\n",
    "    tolink_dataset = tolink_dataset.withColumn('best_candidate_exact', tolink_dataset.result_exact_search['best_candidate_exact'])\n",
    "    tolink_dataset = tolink_dataset.withColumn('sim_best_candidate_exact', tolink_dataset.result_exact_search['sim_best_candidate_exact'])\n",
    "    tolink_dataset = tolink_dataset.withColumn('similarity_exact_candidates', tolink_dataset.result_exact_search['similarity_exact_candidates'])\n",
    "    \n",
    "    tolink_dataset = tolink_dataset.withColumn('sim_best_candidate_exact', F.col('sim_best_candidate_exact').cast('float'))\n",
    "    \n",
    "    # dropping array columns\n",
    "    cols_to_drop = ['result_exact_search']\n",
    "    tolink_dataset = tolink_dataset.drop(*cols_to_drop)\n",
    "    \n",
    "    print(\"\\t[CIDACS-RL] time for exact phase: {} secs\".format(time.time()-start))\n",
    "    return tolink_dataset\n",
    "\n",
    "\n",
    "\n",
    "def cidacs_rl_non_exact_phase(tolink_dataset):\n",
    "    \"\"\"\n",
    "    This function take a dataframe from exact match phase and submit it to a non exact search.\n",
    "    cidacs_rl_non_exact_phase(tolink_dataset) input: \n",
    "    \n",
    "    +--------------------------+--------------------+------------------------+---------------------------+\n",
    "    |                      vars|best_candidate_exact|sim_best_candidate_exact|similarity_exact_candidates|\n",
    "    +--------------------------+--------------------+------------------------+---------------------------+\n",
    "    |       [2, SAMILA SENA, 2]|                null|                    null|                       null|\n",
    "    +--------------------------+--------------------+------------------------+---------------------------+\n",
    "        \n",
    "    cidacs_rl_non_exact_phase(tolink_dataset) output: \n",
    "        \n",
    "        +------------------------+----------------------------+-------------------------------+\n",
    "        |best_candidate_non_exact|sim_best_candidate_non_exact|similarity_exact_non_candidates|\n",
    "        +------------------------+----------------------------+-------------------------------+\n",
    "        |                       7|                        0.94|            {7: 0.94, 3: 0.9...|\n",
    "        +------------------------+----------------------------+-------------------------------+\n",
    "    \n",
    "    At last, this function should return the tolink_dataset with all these columns\n",
    "    \"\"\"\n",
    "    # ------------------------------------ #\n",
    "    # getting relevant values from config\n",
    "    # ------------------------------------ #\n",
    "    \n",
    "    # collecting config json from broadcasted variable\n",
    "    start = time.time()\n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    tolink_id_column = config_['datasets_info']['tolink_dataset']['id_column_name']\n",
    "    \n",
    "    tolink_columns = config_['datasets_info']['tolink_dataset']['columns']\n",
    "    \n",
    "    temp_dir = config_['temp_dir']\n",
    "    \n",
    "    is_debug = config_['debug']\n",
    "    \n",
    "    paralelism = int(config_['datasets_info']['indexed_dataset']['default_paralelism'])\n",
    "\n",
    "    prefix_sl = \"StorageLevel.\"\n",
    "    storage_level = config_['datasets_info']['indexed_dataset']['storage_level']\n",
    "    \n",
    "    write_checkpoint = config_['write_checkpoint']\n",
    "    # ------------------------------------ #\n",
    "    # preparing non exact search\n",
    "    # ------------------------------------ #\n",
    "    \n",
    "    # building linked_from column. Non-null values on sim_best_candidate_exact must be filled \n",
    "    # as 'exact_match', otherwise as 'non_exact_match'.    \n",
    "    filter_isnull = F.col('sim_best_candidate_exact').isNull()\n",
    "    tolink_dataset = tolink_dataset.withColumn('linked_from', F.when(filter_isnull, 'non_exact_match').otherwise('exact_match'))\n",
    "    \n",
    "    # preparing filters for debug and non-debug executions\n",
    "    filter_exact = F.col('linked_from') == 'exact_match'\n",
    "    filter_non_exact = F.col('linked_from') == 'non_exact_match'\n",
    "    \n",
    "    if is_debug == 'false': \n",
    "        # declaring a filtered version of input dataset\n",
    "        tolink_dataset_ = tolink_dataset.filter(filter_non_exact)\n",
    "        # declaring the remainder dataframe\n",
    "        tolink_dataset = tolink_dataset.filter(filter_exact)\n",
    "        \n",
    "        # creating, for remainder dataframe, the cols created in this function to ensure union\n",
    "        tolink_dataset = tolink_dataset.withColumn('best_candidate_non_exact', F.lit(None))\n",
    "        tolink_dataset = tolink_dataset.withColumn('sim_best_candidate_non_exact', F.lit(None))\n",
    "        tolink_dataset = tolink_dataset.withColumn('similarity_non_exact_candidates', F.lit(None))\n",
    "        tolink_dataset = tolink_dataset.withColumn('non_exact_queries', F.lit(None))\n",
    "    else: \n",
    "        # inside dataframe receives the input integrally\n",
    "        tolink_dataset_ = tolink_dataset\n",
    "    \n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('non_exact_queries', udf_build_non_exact_queries(F.col('vars')))\n",
    "\n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('result_non_exact_search', F.explode(F.array(udf_find_elasticsearch_non_exact_best_candidate(F.col('vars'), F.col('non_exact_queries')))))\n",
    "    \n",
    "    if write_checkpoint == 'true':\n",
    "        tolink_dataset_ = optimize_with_checkpoint(\n",
    "            df=tolink_dataset_,\n",
    "            parallelism=paralelism,\n",
    "            storage_level=storage_level,\n",
    "            checkpoint_mode='durable',   # ou 'local'\n",
    "            eager=True,\n",
    "            materialize=True,\n",
    "            materialize_action='count',\n",
    "            prefix_sl=prefix_sl,\n",
    "            unpersist_prev=True\n",
    "        )\n",
    "\n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('best_candidate_non_exact', tolink_dataset_.result_non_exact_search['best_candidate_non_exact'])\n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('sim_best_candidate_non_exact', tolink_dataset_.result_non_exact_search['sim_best_candidate_non_exact'])\n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('similarity_non_exact_candidates', tolink_dataset_.result_non_exact_search['similarity_non_exact_candidates'])\n",
    "    \n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('sim_best_candidate_non_exact', F.col('sim_best_candidate_non_exact').cast('float'))\n",
    "    \n",
    "    cols_to_drop = ['result_non_exact_search']\n",
    "    tolink_dataset_ = tolink_dataset_.drop(*cols_to_drop)\n",
    "    \n",
    "    if is_debug == 'false':\n",
    "        tolink_dataset_ = tolink_dataset_.union(tolink_dataset)\n",
    "    print(\"\\t[CIDACS-RL] time for non-exact phase: {} secs\".format(time.time()-start))\n",
    "    return tolink_dataset_\n",
    "\n",
    "\n",
    "\n",
    "def cidacsrl():\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"[CIDACS-RL] starting at {}\".format(dt_string))\n",
    "    start = time.time()\n",
    "\n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    # getting the auxiliary variables\n",
    "    data_ext = config_['datasets_info']['indexed_dataset']['extension']\n",
    "    data_path = config_['datasets_info']['indexed_dataset']['path']\n",
    "    index_df_response = config_['index_data']\n",
    "    index_name = config_['es_index_name']\n",
    "\n",
    "    if index_df_response == 'yes':\n",
    "        start_ = time.time()\n",
    "        # getting the auxiliary variables\n",
    "        data_ext = config_['datasets_info']['indexed_dataset']['extension']\n",
    "        data_path = config_['datasets_info']['indexed_dataset']['path']\n",
    "        \n",
    "        paralelism = int(config_['datasets_info']['indexed_dataset']['default_paralelism'])\n",
    "        \n",
    "        prefix_sl = \"StorageLevel.\"\n",
    "        storage_level = config_['datasets_info']['indexed_dataset']['storage_level']\n",
    "        \n",
    "        # test the extension of the dataset to properly read it\n",
    "        if data_ext == 'csv':\n",
    "            indexed_dataset = spark.read.csv(data_path, header=True).repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "        elif data_ext == 'parquet':\n",
    "            indexed_dataset = spark.read.parquet(data_path).repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "        else:\n",
    "            print(\"Please make sure the extension for this dataset is set as 'csv' or 'parquet'\")\n",
    "    \n",
    "        # # indexing, at last\n",
    "        index_dataframe(indexed_dataset, index_name)\n",
    "        print(\"[CIDACS-RL] indexing on, it took {} secs\".format(time.time()-start_))\n",
    "    \n",
    "    # getting the auxiliary variables\n",
    "    data_ext = config_['datasets_info']['tolink_dataset']['extension']\n",
    "    data_path = config_['datasets_info']['tolink_dataset']['path']\n",
    "    \n",
    "    paralelism = int(config_['datasets_info']['tolink_dataset']['default_paralelism'])\n",
    "    \n",
    "    prefix_sl = \"StorageLevel.\"\n",
    "    storage_level = config_['datasets_info']['tolink_dataset']['storage_level']\n",
    "\n",
    "    # test the extension of the dataset to properly read it\n",
    "    if data_ext == 'csv':\n",
    "        tolink_dataset = spark.read.csv(data_path, header=True).repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "    elif data_ext == 'parquet':\n",
    "        tolink_dataset = spark.read.parquet(data_path).repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "    else:\n",
    "        print(\"Please make sure the extension for this dataset is set as 'csv' or 'parquet'\")\n",
    "    \n",
    "    tolink_dataset = cidacs_rl_exact_phase(tolink_dataset)\n",
    "    \n",
    "    tolink_dataset = cidacs_rl_non_exact_phase(tolink_dataset)\n",
    "    \n",
    "    tolink_dataset = tolink_dataset.withColumn('final_cidacs_rl_score', \n",
    "                                               F.when(F.col('linked_from') == 'exact_match', F.col('sim_best_candidate_exact'))\\\n",
    "                                                .otherwise(F.col('sim_best_candidate_non_exact')))\n",
    "    \n",
    "    tolink_dataset = tolink_dataset.withColumn('final_cidacs_rl_id', \n",
    "                                               F.when(F.col('linked_from') == 'exact_match', F.col('best_candidate_exact'))\\\n",
    "                                                .otherwise(F.col('best_candidate_non_exact')))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"[CIDACS-RL] finished at {}\".format(dt_string))\n",
    "    print(\"[CIDACS-RL] total time elapsed: {} secs\".format(time.time()-start))\n",
    "    return tolink_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running CIDACS-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'config.json'\n",
    "f = open(config_file)\n",
    "config = json.load(f)\n",
    "\n",
    "# broadcasting config\n",
    "config_bc = sc.broadcast(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_data': 'no',\n",
       " 'es_index_name': 'fd-cidacs-rl',\n",
       " 'es_connect_string': 'http://localhost:9200',\n",
       " 'query_size': 50,\n",
       " 'cutoff_exact_match': '0.95',\n",
       " 'null_value': '99',\n",
       " 'temp_dir': 'hdfs://barravento:9000/data/temp_dataframe/',\n",
       " 'debug': 'false',\n",
       " 'write_checkpoint': 'false',\n",
       " 'datasets_info': {'indexed_dataset': {'path': 'hdfs://barravento:9000/data/synthetic-dataset-A.parquet',\n",
       "   'extension': 'parquet',\n",
       "   'columns': ['id_cidacs_a', 'nome_a', 'nome_mae_a', 'dt_nasc_a', 'sexo_a'],\n",
       "   'id_column_name': 'id_cidacs_a',\n",
       "   'storage_level': 'MEMORY_ONLY',\n",
       "   'default_paralelism': '16'},\n",
       "  'tolink_dataset': {'path': 'hdfs://barravento:9000/data/synthetic-datasets-b-1000.parquet',\n",
       "   'extension': 'parquet',\n",
       "   'columns': ['id_cidacs_b', 'nome_b', 'nome_mae_b', 'dt_nasc_b', 'sexo_b'],\n",
       "   'id_column_name': 'id_cidacs_b',\n",
       "   'storage_level': 'MEMORY_ONLY',\n",
       "   'default_paralelism': '16'},\n",
       "  'result_dataset': {'path': 'hdfs://barravento:9000/data/result/'}},\n",
       " 'comparisons': {'name': {'indexed_col': 'nome_a',\n",
       "   'tolink_col': 'nome_b',\n",
       "   'must_match': 'true',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'true',\n",
       "   'boost': '3.0',\n",
       "   'query_type': 'match',\n",
       "   'similarity': 'jaro_winkler',\n",
       "   'weight': 5.0,\n",
       "   'penalty': 0.02},\n",
       "  'mothers_name': {'indexed_col': 'nome_mae_a',\n",
       "   'tolink_col': 'nome_mae_b',\n",
       "   'must_match': 'true',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'true',\n",
       "   'boost': '2.0',\n",
       "   'query_type': 'match',\n",
       "   'similarity': 'jaro_winkler',\n",
       "   'weight': 5.0,\n",
       "   'penalty': 0.02},\n",
       "  'birthdate': {'indexed_col': 'dt_nasc_a',\n",
       "   'tolink_col': 'dt_nasc_b',\n",
       "   'must_match': 'false',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'false',\n",
       "   'boost': '',\n",
       "   'query_type': 'term',\n",
       "   'similarity': 'hamming',\n",
       "   'weight': 1.0,\n",
       "   'penalty': 0.02},\n",
       "  'sex': {'indexed_col': 'sexo_a',\n",
       "   'tolink_col': 'sexo_b',\n",
       "   'must_match': 'true',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'false',\n",
       "   'boost': '',\n",
       "   'query_type': 'term',\n",
       "   'similarity': 'overlap',\n",
       "   'weight': 3.0,\n",
       "   'penalty': 0.02}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIDACS-RL] starting at 14/08/2025 15:00:56\n",
      "\t[CIDACS-RL] time for exact phase: 0.4350156784057617 secs\n",
      "\t[CIDACS-RL] time for non-exact phase: 0.3547844886779785 secs\n",
      "[CIDACS-RL] finished at 14/08/2025 15:01:00\n",
      "[CIDACS-RL] total time elapsed: 3.851865530014038 secs\n"
     ]
    }
   ],
   "source": [
    "linked_data = cidacsrl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "config_ = config_bc.value\n",
    "\n",
    "output_str_prefix = config_['datasets_info']['result_dataset']['path']\n",
    "output_str_all = \"{}cidacsrl-result_{}.parquet\".format(output_str_prefix, dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_path = config_['datasets_info']['indexed_dataset']['path']\n",
    "indexed_df = spark.read.parquet(indexed_path)\n",
    "linked_data = linked_data.join(indexed_df, \n",
    "                               linked_data.final_cidacs_rl_id == indexed_df.id_cidacs_a, \n",
    "                               \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cidacs_a</th>\n",
       "      <th>nome_a</th>\n",
       "      <th>nome_mae_a</th>\n",
       "      <th>dt_nasc_a</th>\n",
       "      <th>sexo_a</th>\n",
       "      <th>id_cidacs_b</th>\n",
       "      <th>nome_b</th>\n",
       "      <th>nome_mae_b</th>\n",
       "      <th>dt_nasc_b</th>\n",
       "      <th>sexo_b</th>\n",
       "      <th>vars</th>\n",
       "      <th>exact_queries</th>\n",
       "      <th>best_candidate_exact</th>\n",
       "      <th>sim_best_candidate_exact</th>\n",
       "      <th>similarity_exact_candidates</th>\n",
       "      <th>linked_from</th>\n",
       "      <th>non_exact_queries</th>\n",
       "      <th>best_candidate_non_exact</th>\n",
       "      <th>sim_best_candidate_non_exact</th>\n",
       "      <th>similarity_non_exact_candidates</th>\n",
       "      <th>final_cidacs_rl_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14956</td>\n",
       "      <td>LUIZ GABRIEL SANTOS BASTOS</td>\n",
       "      <td>MARIA MARGARIDA SANTOS SILVA</td>\n",
       "      <td>20090627</td>\n",
       "      <td>1</td>\n",
       "      <td>14956</td>\n",
       "      <td>LUIZ GABRIEL SANTOS BASTOS</td>\n",
       "      <td>MARIA MARGARIDA SANTOS SILVA</td>\n",
       "      <td>20090627</td>\n",
       "      <td>1</td>\n",
       "      <td>[14956, LUIZ GABRIEL SANTOS BASTOS, MARIA MARG...</td>\n",
       "      <td>{ \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"LUI...</td>\n",
       "      <td>14956</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{750484=0.8130559916274203, 731128=0.914195526...</td>\n",
       "      <td>exact_match</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>454770</td>\n",
       "      <td>MARCOS ALEXANDRE DE CASTRO DIAS</td>\n",
       "      <td>GIUNY NOQUEIRA DE FIQUEIREDO</td>\n",
       "      <td>20090525</td>\n",
       "      <td>1</td>\n",
       "      <td>454770</td>\n",
       "      <td>MARCOS ALEXANDRE DE CASTRO DIAS</td>\n",
       "      <td>GIUNY NOQUEIRA DE FIQUEIREDO</td>\n",
       "      <td>20090525</td>\n",
       "      <td>1</td>\n",
       "      <td>[454770, MARCOS ALEXANDRE DE CASTRO DIAS, GIUN...</td>\n",
       "      <td>{ \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"MAR...</td>\n",
       "      <td>454770</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{880184=0.7065247653313523, 19260=0.7851056810...</td>\n",
       "      <td>exact_match</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>643154</td>\n",
       "      <td>VALETIM DE OLIVEIRA SANTOS</td>\n",
       "      <td>VARLUCE SANTOS RIBEIRO</td>\n",
       "      <td>20100104</td>\n",
       "      <td>1</td>\n",
       "      <td>643154</td>\n",
       "      <td>VALETIM DE OLIVEIRA SANTOS</td>\n",
       "      <td>VARLUCE SANTOS RIBEIRO</td>\n",
       "      <td>20100104</td>\n",
       "      <td>1</td>\n",
       "      <td>[643154, VALETIM DE OLIVEIRA SANTOS, VARLUCE S...</td>\n",
       "      <td>{ \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"VAL...</td>\n",
       "      <td>643154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{283172=0.850605413105413, 556052=0.8301767676...</td>\n",
       "      <td>exact_match</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_cidacs_a                           nome_a                    nome_mae_a  \\\n",
       "0       14956       LUIZ GABRIEL SANTOS BASTOS  MARIA MARGARIDA SANTOS SILVA   \n",
       "1      454770  MARCOS ALEXANDRE DE CASTRO DIAS  GIUNY NOQUEIRA DE FIQUEIREDO   \n",
       "2      643154       VALETIM DE OLIVEIRA SANTOS        VARLUCE SANTOS RIBEIRO   \n",
       "\n",
       "  dt_nasc_a sexo_a id_cidacs_b                           nome_b  \\\n",
       "0  20090627      1       14956       LUIZ GABRIEL SANTOS BASTOS   \n",
       "1  20090525      1      454770  MARCOS ALEXANDRE DE CASTRO DIAS   \n",
       "2  20100104      1      643154       VALETIM DE OLIVEIRA SANTOS   \n",
       "\n",
       "                     nome_mae_b dt_nasc_b sexo_b  \\\n",
       "0  MARIA MARGARIDA SANTOS SILVA  20090627      1   \n",
       "1  GIUNY NOQUEIRA DE FIQUEIREDO  20090525      1   \n",
       "2        VARLUCE SANTOS RIBEIRO  20100104      1   \n",
       "\n",
       "                                                vars  \\\n",
       "0  [14956, LUIZ GABRIEL SANTOS BASTOS, MARIA MARG...   \n",
       "1  [454770, MARCOS ALEXANDRE DE CASTRO DIAS, GIUN...   \n",
       "2  [643154, VALETIM DE OLIVEIRA SANTOS, VARLUCE S...   \n",
       "\n",
       "                                       exact_queries best_candidate_exact  \\\n",
       "0  { \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"LUI...                14956   \n",
       "1  { \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"MAR...               454770   \n",
       "2  { \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"VAL...               643154   \n",
       "\n",
       "   sim_best_candidate_exact  \\\n",
       "0                       1.0   \n",
       "1                       1.0   \n",
       "2                       1.0   \n",
       "\n",
       "                         similarity_exact_candidates  linked_from  \\\n",
       "0  {750484=0.8130559916274203, 731128=0.914195526...  exact_match   \n",
       "1  {880184=0.7065247653313523, 19260=0.7851056810...  exact_match   \n",
       "2  {283172=0.850605413105413, 556052=0.8301767676...  exact_match   \n",
       "\n",
       "  non_exact_queries best_candidate_non_exact  sim_best_candidate_non_exact  \\\n",
       "0              None                     None                           NaN   \n",
       "1              None                     None                           NaN   \n",
       "2              None                     None                           NaN   \n",
       "\n",
       "  similarity_non_exact_candidates  final_cidacs_rl_score  \n",
       "0                            None                    1.0  \n",
       "1                            None                    1.0  \n",
       "2                            None                    1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_a = config_['datasets_info']['indexed_dataset']['columns']\n",
    "columns_b = config_['datasets_info']['tolink_dataset']['columns']\n",
    "seen = set()\n",
    "merged_entity_cols = []\n",
    "for cols in (columns_a, columns_b):\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            seen.add(c)\n",
    "            if c in linked_data.columns:\n",
    "                merged_entity_cols.append(c)\n",
    "                \n",
    "fixed_cols = [\n",
    "    'vars',\n",
    "    'exact_queries', 'best_candidate_exact', 'sim_best_candidate_exact', 'similarity_exact_candidates',\n",
    "    'linked_from',\n",
    "    'non_exact_queries', 'best_candidate_non_exact', 'sim_best_candidate_non_exact', 'similarity_non_exact_candidates',\n",
    "    'final_cidacs_rl_score'\n",
    "]\n",
    "\n",
    "fixed_cols_present = [c for c in fixed_cols if c in linked_data.columns]\n",
    "\n",
    "select_cols = merged_entity_cols + fixed_cols_present\n",
    "\n",
    "linked_data = linked_data.select(*select_cols).sort(F.desc('final_cidacs_rl_score'))\n",
    "linked_data.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linked_data.filter(F.col('linked_from') == 'non_exact_match') \\\n",
    "#            .sort(F.desc('final_cidacs_rl_score')).limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_data.write.parquet(output_str_all, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total de execução: 159.88978672027588 secs\n"
     ]
    }
   ],
   "source": [
    "print(\"Tempo total de execução: {} secs\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|    linked_from|count|\n",
      "+---------------+-----+\n",
      "|non_exact_match|  458|\n",
      "|    exact_match|  542|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linked_data.select('linked_from').groupBy('linked_from').count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
