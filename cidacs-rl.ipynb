{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import jellyfish\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option(\"display.max_columns\", 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CIDACSRL\") \\\n",
    "    .master(\"spark://barravento:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.elasticsearch:elasticsearch-spark-30_2.12:8.1.3\") \\\n",
    "    .config(\"spark.es.nodes\", \"barravento\") \\\n",
    "    .config(\"spark.es.port\", \"9200\") \\\n",
    "    .config(\"spark.es.nodes.wan.only\", \"false\") \\\n",
    "    .config(\"spark.es.resource\", \"dbb2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ES functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_match_cols_and_values(vars_col, query_type, add_id_col):\n",
    "    \"\"\"\n",
    "    query_type must be 'exact' for building exact queries or 'general' for any else query and comparison.\n",
    "    \"\"\"\n",
    "    config_ = config_bc.value\n",
    "    # getting names of indexed columns\n",
    "    indexed_id_column = config_['datasets_info']['indexed_dataset']['id_column_name']\n",
    "    \n",
    "    indexed_cols = config_['datasets_info']['indexed_dataset']['columns']\n",
    "    \n",
    "#     if query_type == 'general':\n",
    "#         indexed_cols = [x for x in indexed_cols if x != indexed_id_column]\n",
    "        \n",
    "    # notice that we are linking indexed keys with tolink values\n",
    "    # the keys will be used to set which field will be fetched on es\n",
    "    # the values will be used as search content\n",
    "    tolink_cols_dict = dict(zip(indexed_cols, vars_col))\n",
    "    \n",
    "    if add_id_col == False:\n",
    "        tolink_cols_dict.pop(indexed_id_column, None)\n",
    "    \n",
    "    if query_type == 'general':\n",
    "        return tolink_cols_dict\n",
    "    elif query_type == 'exact':\n",
    "        # finding which are the columns used on exact match step\n",
    "        indexed_exact_match_vars = [indexed_id_column] + [config_['comparisons'][x]['indexed_col'] for x in config_['comparisons'] if config_['comparisons'][x]['must_match'] == 'true']\n",
    "        non_exact_match_cols = list(set(indexed_cols) - set(indexed_exact_match_vars))\n",
    "        # deleting those columns of non-exact match\n",
    "        [tolink_cols_dict.pop(x, None) for x in non_exact_match_cols]\n",
    "        \n",
    "        return tolink_cols_dict\n",
    "    else: \n",
    "        print(\"Please use 'general' or 'exact' as query_type input\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def index_dataframe(dataframe, es_index_name):\n",
    "    # creating new index\n",
    "    dataframe.write.format(\"org.elasticsearch.spark.sql\") \\\n",
    "                 .option(\"es.resource\", es_index_name).mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_exact_queries(vars_col): \n",
    "    \"\"\"\n",
    "    Let us suppose the following values:\n",
    "    vars_col = ['ROBESPIERRE PITA', '1987-05-05', '1', 'Mari Santos']\n",
    "    indexed_cols = ['name', 'birthdate', 'sex', 'mothers_name']\n",
    "    query_size = 10\n",
    "    \n",
    "    and only the first two attributes are assigned to exact match.\n",
    "    So, the resulting query column would be: \n",
    "    '{ \"size\": \"50\", \"query\": \n",
    "                    { \"bool\": { \"must\": [ \n",
    "                                {\"match\": {\"name\":\"ROBESPIERRE PITA\"}},\n",
    "                                {\"match\": {\"birthdate\":\"19870505\"}}] } } }'\n",
    "    Requirements: \n",
    "    - All values on vars_col must be converted into string\n",
    "    - All the hyphens symbols must be taken from date type used to search (e.g. 1987-05-05 must be converted to 19870505)\n",
    "    - The config json must be available as a broadcast through sc.broadcast() function.\n",
    "    - The names of indexed columns must be correctly filled. \n",
    "    \"\"\"\n",
    "    config_ = config_bc.value\n",
    "    query_size = config_['query_size']\n",
    "    \n",
    "    tolink_cols_dict = get_match_cols_and_values(vars_col, 'exact', False)\n",
    "    \n",
    "    # -------------------------------------------- #\n",
    "    #   starting the building of query string      #\n",
    "    # -------------------------------------------- #\n",
    "    # setting the preffix and suffix of query core\n",
    "    prefix_ = \"\"\"{\"match\": {\"\"\"\n",
    "    suffix_ = \"\"\"}}\"\"\"\n",
    "    \n",
    "    # filling the query core with all indexed columns and values from vars_col\n",
    "    strings = []\n",
    "    for col in list(tolink_cols_dict.keys()):\n",
    "        string = str(prefix_) + \"\\\"\" + str(col) + \"\\\"\" + \":\" + \"\\\"\" +  str(tolink_cols_dict[col]) + \"\\\"\" + str(suffix_)\n",
    "        strings.append(string)\n",
    "    \n",
    "    # building the query core. \n",
    "    # Should be like: {\"match\": {\"name\":\"ROBESPIERRE PITA\"}}, {\"birthdate\": {\"name\":\"1987-05-05\"}}\n",
    "    line = ','.join(strings)\n",
    "    \n",
    "    # Finally the final query string\n",
    "    complete_query = \"\"\"{ \"bool\": { \"must\": [ %s ] } }\"\"\" % (line)\n",
    "    # CHANGELOG: to accomplish with new syntax from ES 8.x, we need to change the complete query from\n",
    "    # complete_query = \"\"\"{ \"size\": \"%s\", \"query\": { \"bool\": { \"must\": [ %s ] } } }\"\"\" % (query_size,line)\n",
    "    # to\n",
    "    # complete_query = \"\"\"{ \"bool\": { \"must\": [ %s ] } } }\"\"\" % (line)\n",
    "    # read more in: https://github.com/elastic/elasticsearch-py/issues/1698\n",
    "    #               https://www.elastic.co/guide/en/elasticsearch/client/python-api/8.1/examples.html\n",
    "    \n",
    "    return complete_query\n",
    "udf_build_exact_queries = F.udf(build_exact_queries, StringType()) \n",
    "\n",
    "def build_non_exact_queries(vars_col): \n",
    "    \"\"\"\n",
    "    Let us suppose the following values:\n",
    "    vars_col = ['ROBESPIERRE PITA', '1987-05-05', '1', 'Mari Santos']\n",
    "    indexed_cols = ['name', 'birthdate', 'sex', 'mothers_name']\n",
    "    query_size = 10\n",
    "    \n",
    "    and only the first two attributes are assigned to exact match.\n",
    "    So, the resulting query column would be: \n",
    "    '{\"bool\": { \n",
    "                 \"should\": [ \n",
    "                     {'match': {'nome_a': {'query': 'ROBESPIERRE PITA', 'fuzziness':'AUTO', 'operator':'or', 'boost':'3.0'}}},\n",
    "                     {\"match\": {\"birthdate\":\"19870505\"}} ] } } }\n",
    "                     {\"term\": {\"sexo_a\":\"1\"}} ] } } }'\n",
    "    Requirements: \n",
    "    - All values on vars_col must be converted into string\n",
    "    - All the hyphens symbols must be taken from date type used to search (e.g. 1987-05-05 must be converted to 19870505)\n",
    "    - The config json must be available as a broadcast through sc.broadcast() function.\n",
    "    - The names of indexed columns must be correctly filled. \n",
    "    \"\"\"\n",
    "    config_ = config_bc.value\n",
    "    query_size = config_['query_size']\n",
    "    \n",
    "    tolink_cols_dict = get_match_cols_and_values(vars_col, 'exact', False)\n",
    "    \n",
    "    # -------------------------------------------- #\n",
    "    #   starting the building of query string      #\n",
    "    # -------------------------------------------- #\n",
    "    \n",
    "    # filling the query core with all indexed columns and values from vars_col\n",
    "    comparisons = [config['comparisons'][x] for x in config['comparisons']]\n",
    "    strings = []\n",
    "    for col in list(tolink_cols_dict.keys()):\n",
    "        query_col_instructions = [x for x in comparisons if x['indexed_col'] == col][0]\n",
    "        query_type = str(query_col_instructions['query_type'])\n",
    "        prefix_ = \"\"\"{\"%s\": {\"\"\" % query_type\n",
    "        suffix_ = \"\"\"}}\"\"\"\n",
    "\n",
    "        if query_col_instructions['should_match'] == 'true':\n",
    "            if query_col_instructions['is_fuzzy'] == 'true':\n",
    "                boost = str(query_col_instructions['boost'])\n",
    "                string = str(prefix_) + \"\\\"\" + str(col) + \"\\\"\" + \":\" + \" { \\\"query\\\" : \\\"\" +  str(tolink_cols_dict[col]) + \"\\\"\" + \", \\\"fuzziness\\\":\\\"AUTO\\\", \\\"operator\\\":\\\"or\\\", \\\"boost\\\":\\\"\" + boost + \"\\\" }\" + str(suffix_)\n",
    "                \n",
    "            if query_col_instructions['is_fuzzy'] == 'false':\n",
    "                string = str(prefix_) + \"\\\"\" + str(col) + \"\\\"\" + \":\" + \"\\\"\" +  str(tolink_cols_dict[col]) + \"\\\"\" + str(suffix_)\n",
    "        strings.append(string)\n",
    "    \n",
    "    # building the query core. \n",
    "    # is_fuzzy = 'true' should be like: {\"match\": {\"name\":\"ROBESPIERRE PITA\", \"fuzziness\":\"AUTO\", \"operator\":\"or\", \"boost\":\"3.0\"}}, {\"term\": {\"dt_nasc_a\":\"20070816\"}}\n",
    "    line = ','.join(strings)\n",
    "    \n",
    "    # Finally the final query string\n",
    "    complete_query = \"\"\"{ \"bool\": { \"should\": [ %s ] } }\"\"\" % (line)\n",
    "    # CHANGELOG: to accomplish with new syntax from ES 8.x, we need to change the complete query from\n",
    "    # complete_query = \"\"\"{ \"size\": \"%s\", \"query\": { \"bool\": { \"should\": [ %s ] } } }\"\"\" % (query_size,line)\n",
    "    # to\n",
    "    # complete_query = \"\"\"{ \"bool\": { \"should\": [ %s ] } } }\"\"\" % (query_size,line)\n",
    "    # read more in: https://github.com/elastic/elasticsearch-py/issues/1698\n",
    "    #               https://www.elastic.co/guide/en/elasticsearch/client/python-api/8.1/examples.html\n",
    "    \n",
    "    return complete_query\n",
    "udf_build_non_exact_queries = F.udf(build_non_exact_queries, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finding matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_elasticsearch_exact_best_candidate(vars_col, exact_queries_col):\n",
    "    \"\"\"\n",
    "    Let us suppose a column with the following query:\n",
    "    \n",
    "    '{ \"bool\": { \"must\": [ \n",
    "                    {\"match\": {\"name\":\"ROBESPIERRE PITA\"}},\n",
    "                    {\"match\": {\"birthdate\":\"19870505\"}}] } }'\n",
    "    \n",
    "    so, this function must return a dict with N results like: \n",
    "        {'_index': 'test', '_type': '_doc', '_id': 'aaabbbccc', '_score': 43.9280841,\n",
    "        '_source': {'name': 'ROBESPIERRE PITA', 'birthdate': '19870505', 'other_col': 'other_value'}},\n",
    "    \n",
    "    being N the query_size value set on config, you can see this number on the 'size' field of the query.\n",
    "    \n",
    "    This result can now be used to compute the proper similarity and pick the \n",
    "    best candidate for each record\n",
    "    \"\"\"\n",
    "    from elasticsearch import Elasticsearch\n",
    "    from ast import literal_eval\n",
    "\n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    es_connect_string = config_['es_connect_string']\n",
    "    es_index_name = config_['es_index_name']\n",
    "    query_size = config_['query_size']\n",
    "    \n",
    "    es = Elasticsearch(es_connect_string)\n",
    "    \n",
    "    exact_queries_col = literal_eval(exact_queries_col)\n",
    "    # CHANGELOG: To accomplish with the syntax of ES 8.x we need to add the lines\n",
    "    # from ast import literal_eval\n",
    "    # exact_queries_col = literal_eval(exact_queries_col)\n",
    "    # String typed query contents does not meet the new requisits, rainsing the following error (even when they are properly written): \n",
    "    # # elasticsearch.BadRequestError: BadRequestError(400, 'parsing_exception', 'Unknown key for a VALUE_STRING in [query].')\n",
    "    \n",
    "    candidates = es.search(index=es_index_name, query=exact_queries_col, size=query_size)['hits']['hits']\n",
    "    # CHANGELOG: To accomplish with the syntax of ES 8.x we need to change the line from:\n",
    "    # candidates = es.search(index=es_index_name, body=exact_queries_col)['hits']['hits']\n",
    "    # to \n",
    "    # candidates = es.search(index=es_index_name, query=exact_queries_col, size=query_size)['hits']['hits']\n",
    "    # This could avoid the following errors: \n",
    "    # # ValueError: Couldn't merge 'body' with other parameters as it wasn't a mapping. Instead of using 'body' use individual API parameters\n",
    "    # # elasticsearch.BadRequestError: BadRequestError(400, 'parsing_exception', 'Unknown key for a VALUE_STRING in [query].')\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        best_score_id, best_score_value, scores = 'null', 'null', 'null'\n",
    "        return T.Row('best_candidate_exact', 'sim_best_candidate_exact', 'similarity_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "    else:\n",
    "        cols_and_values = get_match_cols_and_values(vars_col, 'general', True)\n",
    "        best_score_id, best_score_value, scores = find_best_candidates(cols_and_values, candidates)\n",
    "\n",
    "        if float(best_score_value) >= float(config_['cutoff_exact_match']):\n",
    "            return T.Row('best_candidate_exact', 'sim_best_candidate_exact', 'similarity_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "        else: \n",
    "            best_score_id, best_score_value, scores = 'null', 'null', 'null'\n",
    "            return T.Row('best_candidate_exact', 'sim_best_candidate_exact', 'similarity_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "\n",
    "schema = StructType([StructField(\"best_candidate_exact\", StringType(), False), \n",
    "                     StructField(\"sim_best_candidate_exact\", StringType(), False), \n",
    "                     StructField(\"similarity_exact_candidates\", StringType(), False)])\n",
    "udf_find_elasticsearch_exact_best_candidate = F.udf(find_elasticsearch_exact_best_candidate, schema)\n",
    "\n",
    "\n",
    "def find_elasticsearch_non_exact_best_candidate(vars_col, non_exact_queries_col):\n",
    "    \"\"\"\n",
    "    Let us suppose a column with the following query:\n",
    "    \n",
    "    '{ \"bool\": { \n",
    "                 \"should\": [ \n",
    "                     {\"match\": {\"nome_a\":\"ROBESPIERRE PITA\", \"fuzziness\":\"AUTO\", \"operator\":\"or\", \"boost\":\"3.0\"}},\n",
    "                     {\"match\": {\"birthdate\":\"19870505\"}} ] } }'\n",
    "    \n",
    "    so, this function must return a dict with N results like: \n",
    "        {'_index': 'test', '_type': '_doc', '_id': 'aaabbbccc', '_score': 43.9280841,\n",
    "        '_source': {'name': 'ROBESPIERRE PITA', 'birthdate': '19870505', 'other_col': 'other_value'}},\n",
    "    \n",
    "    being N the query_size value set on config, you can see this number on the 'size' field of the query.\n",
    "    \n",
    "    This result can now be used to compute the proper similarity and pick the \n",
    "    best candidate for each record\n",
    "    \"\"\"\n",
    "    from elasticsearch import Elasticsearch\n",
    "    from ast import literal_eval\n",
    "    \n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    es_connect_string = config_['es_connect_string']\n",
    "    es_index_name = config_['es_index_name']\n",
    "    query_size = config_['query_size']\n",
    "    \n",
    "    es = Elasticsearch(es_connect_string)\n",
    "    \n",
    "    non_exact_queries_col = literal_eval(non_exact_queries_col)\n",
    "    # CHANGELOG: To accomplish with the syntax of ES 8.x we need to add the lines\n",
    "    # from ast import literal_eval\n",
    "    # exact_queries_col = literal_eval(exact_queries_col)\n",
    "    # String typed query contents does not meet the new requisits, rainsing the following error (even when they are properly written): \n",
    "    # # elasticsearch.BadRequestError: BadRequestError(400, 'parsing_exception', 'Unknown key for a VALUE_STRING in [query].')\n",
    "    \n",
    "    \n",
    "    candidates = es.search(index=es_index_name, query=non_exact_queries_col, size=query_size)['hits']['hits']\n",
    "    # CHANGELOG: To accomplish with the syntax of ES 8.x we need to change the line from:\n",
    "    # candidates = es.search(index=es_index_name, body=non_exact_queries_col)['hits']['hits']\n",
    "    # to \n",
    "    # candidates = es.search(index=es_index_name, query=non_exact_queries_col, size=query_size)['hits']['hits']\n",
    "    # This could avoid the following errors: \n",
    "    # # ValueError: Couldn't merge 'body' with other parameters as it wasn't a mapping. Instead of using 'body' use individual API parameters\n",
    "    # # elasticsearch.BadRequestError: BadRequestError(400, 'parsing_exception', 'Unknown key for a VALUE_STRING in [query].')\n",
    "    \n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        best_score_id, best_score_value, scores = 'null', 'null', 'null'\n",
    "        return T.Row('best_candidate_non_exact', 'sim_best_candidate_non_exact', 'similarity_non_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "    else:\n",
    "        cols_and_values = get_match_cols_and_values(vars_col, 'general', True)\n",
    "        best_score_id, best_score_value, scores = find_best_candidates(cols_and_values, candidates)\n",
    "        return T.Row('best_candidate_non_exact', 'sim_best_candidate_non_exact', 'similarity_non_exact_candidates')(best_score_id, best_score_value, scores)\n",
    "        \n",
    "schema = StructType([StructField(\"best_candidate_non_exact\", StringType(), False), \n",
    "                     StructField(\"sim_best_candidate_non_exact\", StringType(), False), \n",
    "                     StructField(\"similarity_non_exact_candidates\", StringType(), False)])\n",
    "udf_find_elasticsearch_non_exact_best_candidate = F.udf(find_elasticsearch_non_exact_best_candidate, schema)\n",
    "\n",
    "\n",
    "\n",
    "def find_best_candidates(cols_and_values, candidates):\n",
    "    \n",
    "    config_ = config_bc.value\n",
    "    indexed_id_col = config_['datasets_info']['indexed_dataset']['id_column_name']\n",
    "    id_value = cols_and_values[indexed_id_col]\n",
    "    scores = {}\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        candidate_id = candidate['_source'][indexed_id_col]\n",
    "        sim_candidate = []\n",
    "\n",
    "        for col_and_value in list(cols_and_values.keys()):\n",
    "            if col_and_value != indexed_id_col:\n",
    "                comparison_info = [config_['comparisons'][x] for x in config_['comparisons'] if config_['comparisons'][x]['indexed_col'] == col_and_value][0]\n",
    "                n_comparisons = len(config_['comparisons'].keys())\n",
    "\n",
    "                sim_for_pair_of_cols = similarity_hub(n_comparisons, comparison_info, cols_and_values[col_and_value], candidate['_source'][col_and_value])\n",
    "\n",
    "                sim_candidate.append(sim_for_pair_of_cols)\n",
    "\n",
    "        score_max = sum([float(config_['comparisons'][x]['weight']) for x in config_['comparisons']])\n",
    "        score = (sum(sim_candidate))/score_max\n",
    "    \n",
    "        scores[candidate_id] = score\n",
    "    \n",
    "    if len(scores) > 0:\n",
    "        best_score_id = max(scores, key=scores.get)\n",
    "        best_score_value = scores[best_score_id]\n",
    "    else: \n",
    "        best_score_id = 'null'\n",
    "        best_score_value = '0.0'\n",
    "        scores = '{}'\n",
    "    return best_score_id, best_score_value, scores\n",
    "    \n",
    "    \n",
    "def similarity_hub(n_comparisons, comparison_info, col_and_value, candidate):\n",
    "    \"\"\"\n",
    "    Currently the CIDACS-RL uses overlap for categorical data, jaro_winkler for names and hamming for dates.\n",
    "    \"\"\"\n",
    "    import jellyfish\n",
    "    \n",
    "    # getting relevant information for this pair of values\n",
    "    config_ = config_bc.value\n",
    "#     score_max = sum([float(config_['comparisons'][x]['weight']) for x in config_['comparisons']])\n",
    "    similarity = 0.0\n",
    "    weight = float(comparison_info['weight'])\n",
    "    penalty = float(comparison_info['penalty'])\n",
    "    \n",
    "    # first, test if some value are missing\n",
    "    if (candidate == config_['null_value']) or (col_and_value == config_['null_value'])\\\n",
    "        or (candidate == \"\") or (col_and_value == \"\") or (candidate == None) or (col_and_value == None):\n",
    "        similarity = similarity - penalty\n",
    "    else: \n",
    "        sim_type = comparison_info['similarity']\n",
    "        if (sim_type == 'overlap') and (col_and_value == candidate):\n",
    "            similarity += (1.0) * weight\n",
    "            return similarity\n",
    "        elif (sim_type == 'overlap') and (col_and_value != candidate):\n",
    "            similarity += 0.0\n",
    "            return similarity\n",
    "        elif sim_type == 'jaro_winkler':\n",
    "            similarity += jellyfish.jaro_winkler(col_and_value, candidate) * weight\n",
    "        elif sim_type == 'hamming':\n",
    "            max_size = max(len(col_and_value), len(candidate))\n",
    "            similarity += 1.0 - float(jellyfish.hamming_distance(col_and_value, candidate)/max_size) * weight\n",
    "        else: \n",
    "            print('Please inform valid similarities for cidacs-rl')\n",
    "        \n",
    "        similarity = similarity\n",
    "    return similarity    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cidacs_rl_exact_phase(tolink_dataset):\n",
    "    \"\"\"\n",
    "    This function take a dataframe to link with an indexed dataframe on elasticsearch.\n",
    "    It consists in three main steps: \n",
    "        1) The first step consists in create an array column from a set of columns used on integration\n",
    "        \n",
    "        withColumn('vars', F.array(tolink_cols)) input: \n",
    "        +-----------+--------------------+------+\n",
    "        |id_cidacs_b|                nome|  sexo|\n",
    "        +-----------+--------------------+------+\n",
    "        |          0|    ROBESPIERRE PITA|     1|\n",
    "        +-----------+--------------------+------+\n",
    "        \n",
    "        withColumn('vars', F.array(tolink_cols)) output: \n",
    "        +-----------+--------------------+------+--------------------------+\n",
    "        |id_cidacs_b|                nome|  sexo|                      vars|\n",
    "        +-----------+--------------------+------+--------------------------+\n",
    "        |        614|    ROBESPIERRE PITA|     1|  [0, ROBESPIERRE PITA, 1]|\n",
    "        +-----------+--------------------+------+--------------------------+\n",
    "        \n",
    "        2) The second step will take the new array col as input and build exact queries:\n",
    "        \n",
    "        udf_build_exact_queries(F.col('vars')) output:\n",
    "        \n",
    "        { \"size\": \"50\",\n",
    "            \"query\": { \"bool\": \n",
    "            { \"must\": [ \n",
    "                {\"match\": {\"nome_a\":\"ROBESPIERRE PITA\"}},\n",
    "                {\"match\": {\"sexo_a\":\"1\"}} ] } } }\n",
    "        \n",
    "        +-----------+-----------------+------+--------------------------+----------------+\n",
    "        |id_cidacs_b|             nome|  sexo|                      vars|     exact_query|\n",
    "        +-----------+-----------------+------+--------------------------+----------------+\n",
    "        |        614|    ROBESPIERR...|     1|  [0, ROBESPIERRE PITA, 1]| { \"size\": \"5...|\n",
    "        +-----------+-----------------+------+--------------------------+----------------+\n",
    "        \n",
    "        3) Finally, a udf should generate 3 new columns with the best candidate id, the similarity with \n",
    "           this best candidate, and the set of candidates scores. \n",
    "        \n",
    "        +--------------------+------------------------+---------------------------+\n",
    "        |best_candidate_exact|sim_best_candidate_exact|similarity_exact_candidates|\n",
    "        +--------------------+------------------------+---------------------------+\n",
    "        |                 614|                       1|        {614: 1, 34: 0.8...|\n",
    "        +--------------------+------------------------+---------------------------+\n",
    "        \n",
    "    At last, this function should return the tolink_dataset with all these columns\n",
    "    \"\"\" \n",
    "    start = time.time()\n",
    "    # ------------------------------------ #\n",
    "    # getting relevant values from config\n",
    "    # ------------------------------------ #\n",
    "    \n",
    "    # collecting config json from broadcasted variable\n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    tolink_id_column = config_['datasets_info']['tolink_dataset']['id_column_name']\n",
    "    \n",
    "    tolink_columns = config_['datasets_info']['tolink_dataset']['columns']\n",
    "    \n",
    "    temp_dir = config['temp_dir']\n",
    "    \n",
    "    paralelism = int(config_['datasets_info']['indexed_dataset']['default_paralelism'])\n",
    "\n",
    "    prefix_sl = \"StorageLevel.\"\n",
    "    storage_level = config['datasets_info']['indexed_dataset']['storage_level']\n",
    "    \n",
    "    write_checkpoint = config_['write_checkpoint']\n",
    "    \n",
    "    # ------------------------------------ #\n",
    "    # preparing exact search\n",
    "    # ------------------------------------ #\n",
    "    # selecting columns\n",
    "    tolink_dataset = tolink_dataset.select(tolink_columns)\n",
    "    # building array of variable values\n",
    "    tolink_dataset = tolink_dataset.withColumn('vars', F.array(tolink_columns))\n",
    "    # building exact queries\n",
    "    tolink_dataset = tolink_dataset.withColumn('exact_queries', udf_build_exact_queries(F.col('vars')))\n",
    "    # finding the best candidate for each tolink record\n",
    "    tolink_dataset = tolink_dataset.withColumn('result_exact_search', F.explode(F.array(udf_find_elasticsearch_exact_best_candidate(F.col('vars'), F.col('exact_queries')))))\n",
    "    \n",
    "    if write_checkpoint == 'true':\n",
    "        # writing temporary data from this point helps to reset the DAG and improve performance\n",
    "        tolink_dataset.write.parquet(temp_dir+'result_exact_search.parquet', mode='overwrite')\n",
    "        tolink_dataset = spark.read.parquet(temp_dir+'result_exact_search.parquet').repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "    \n",
    "    # exploding array columns from the last function into 4 atomic cols\n",
    "    tolink_dataset = tolink_dataset.withColumn('best_candidate_exact', tolink_dataset.result_exact_search['best_candidate_exact'])\n",
    "    tolink_dataset = tolink_dataset.withColumn('sim_best_candidate_exact', tolink_dataset.result_exact_search['sim_best_candidate_exact'])\n",
    "    tolink_dataset = tolink_dataset.withColumn('similarity_exact_candidates', tolink_dataset.result_exact_search['similarity_exact_candidates'])\n",
    "    \n",
    "    tolink_dataset = tolink_dataset.withColumn('sim_best_candidate_exact', F.col('sim_best_candidate_exact').cast('float'))\n",
    "    \n",
    "    # dropping array columns\n",
    "    cols_to_drop = ['result_exact_search']\n",
    "    tolink_dataset = tolink_dataset.drop(*cols_to_drop)\n",
    "    \n",
    "    print(\"\\t[CIDACS-RL] time for exact phase: {} secs\".format(time.time()-start))\n",
    "    return tolink_dataset\n",
    "\n",
    "\n",
    "\n",
    "def cidacs_rl_non_exact_phase(tolink_dataset):\n",
    "    \"\"\"\n",
    "    This function take a dataframe from exact match phase and submit it to a non exact search.\n",
    "    cidacs_rl_non_exact_phase(tolink_dataset) input: \n",
    "    \n",
    "    +--------------------------+--------------------+------------------------+---------------------------+\n",
    "    |                      vars|best_candidate_exact|sim_best_candidate_exact|similarity_exact_candidates|\n",
    "    +--------------------------+--------------------+------------------------+---------------------------+\n",
    "    |       [2, SAMILA SENA, 2]|                null|                    null|                       null|\n",
    "    +--------------------------+--------------------+------------------------+---------------------------+\n",
    "        \n",
    "    cidacs_rl_non_exact_phase(tolink_dataset) output: \n",
    "        \n",
    "        +------------------------+----------------------------+-------------------------------+\n",
    "        |best_candidate_non_exact|sim_best_candidate_non_exact|similarity_exact_non_candidates|\n",
    "        +------------------------+----------------------------+-------------------------------+\n",
    "        |                       7|                        0.94|            {7: 0.94, 3: 0.9...|\n",
    "        +------------------------+----------------------------+-------------------------------+\n",
    "    \n",
    "    At last, this function should return the tolink_dataset with all these columns\n",
    "    \"\"\"\n",
    "    # ------------------------------------ #\n",
    "    # getting relevant values from config\n",
    "    # ------------------------------------ #\n",
    "    \n",
    "    # collecting config json from broadcasted variable\n",
    "    start = time.time()\n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    tolink_id_column = config_['datasets_info']['tolink_dataset']['id_column_name']\n",
    "    \n",
    "    tolink_columns = config_['datasets_info']['tolink_dataset']['columns']\n",
    "    \n",
    "    temp_dir = config_['temp_dir']\n",
    "    \n",
    "    is_debug = config_['debug']\n",
    "    \n",
    "    paralelism = int(config_['datasets_info']['indexed_dataset']['default_paralelism'])\n",
    "\n",
    "    prefix_sl = \"StorageLevel.\"\n",
    "    storage_level = config['datasets_info']['indexed_dataset']['storage_level']\n",
    "    \n",
    "    write_checkpoint = config_['write_checkpoint']\n",
    "    # ------------------------------------ #\n",
    "    # preparing non exact search\n",
    "    # ------------------------------------ #\n",
    "    \n",
    "    # building linked_from column. Non-null values on sim_best_candidate_exact must be filled \n",
    "    # as 'exact_match', otherwise as 'non_exact_match'.    \n",
    "    filter_isnull = F.col('sim_best_candidate_exact').isNull()\n",
    "    tolink_dataset = tolink_dataset.withColumn('linked_from', F.when(filter_isnull, 'non_exact_match').otherwise('exact_match'))\n",
    "    \n",
    "    # preparing filters for debug and non-debug executions\n",
    "    filter_exact = F.col('linked_from') == 'exact_match'\n",
    "    filter_non_exact = F.col('linked_from') == 'non_exact_match'\n",
    "    \n",
    "    if is_debug == 'false': \n",
    "        # declaring a filtered version of input dataset\n",
    "        tolink_dataset_ = tolink_dataset.filter(filter_non_exact)\n",
    "        # declaring the remainder dataframe\n",
    "        tolink_dataset = tolink_dataset.filter(filter_exact)\n",
    "        \n",
    "        # creating, for remainder dataframe, the cols created in this function to ensure union\n",
    "        tolink_dataset = tolink_dataset.withColumn('best_candidate_non_exact', F.lit(None))\n",
    "        tolink_dataset = tolink_dataset.withColumn('sim_best_candidate_non_exact', F.lit(None))\n",
    "        tolink_dataset = tolink_dataset.withColumn('similarity_non_exact_candidates', F.lit(None))\n",
    "        tolink_dataset = tolink_dataset.withColumn('non_exact_queries', F.lit(None))\n",
    "    else: \n",
    "        # inside dataframe receives the input integrally\n",
    "        tolink_dataset_ = tolink_dataset\n",
    "    \n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('non_exact_queries', udf_build_non_exact_queries(F.col('vars')))\n",
    "\n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('result_non_exact_search', F.explode(F.array(udf_find_elasticsearch_non_exact_best_candidate(F.col('vars'), F.col('non_exact_queries')))))\n",
    "    \n",
    "    if write_checkpoint == 'true':\n",
    "        # writing temporary data from this point helps to reset the DAG and improve performance\n",
    "        tolink_dataset_.write.parquet(temp_dir+'result_non_exact_search.parquet', mode='overwrite')\n",
    "        tolink_dataset_ = spark.read.parquet(temp_dir+'result_non_exact_search.parquet').repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "\n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('best_candidate_non_exact', tolink_dataset_.result_non_exact_search['best_candidate_non_exact'])\n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('sim_best_candidate_non_exact', tolink_dataset_.result_non_exact_search['sim_best_candidate_non_exact'])\n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('similarity_non_exact_candidates', tolink_dataset_.result_non_exact_search['similarity_non_exact_candidates'])\n",
    "    \n",
    "    tolink_dataset_ = tolink_dataset_.withColumn('sim_best_candidate_non_exact', F.col('sim_best_candidate_non_exact').cast('float'))\n",
    "    \n",
    "    cols_to_drop = ['result_non_exact_search']\n",
    "    tolink_dataset_ = tolink_dataset_.drop(*cols_to_drop)\n",
    "    \n",
    "    if is_debug == 'false':\n",
    "        tolink_dataset_ = tolink_dataset_.union(tolink_dataset)\n",
    "    print(\"\\t[CIDACS-RL] time for non-exact phase: {} secs\".format(time.time()-start))\n",
    "    return tolink_dataset_\n",
    "\n",
    "\n",
    "\n",
    "def cidacsrl():\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"[CIDACS-RL] starting at {}\".format(dt_string))\n",
    "    start = time.time()\n",
    "\n",
    "    config_ = config_bc.value\n",
    "    \n",
    "    # getting the auxiliary variables\n",
    "    data_ext = config_['datasets_info']['indexed_dataset']['extension']\n",
    "    data_path = config_['datasets_info']['indexed_dataset']['path']\n",
    "    index_df_response = config_['index_data']\n",
    "    index_name = config_['es_index_name']\n",
    "\n",
    "    if index_df_response == 'yes':\n",
    "        start_ = time.time()\n",
    "        # getting the auxiliary variables\n",
    "        data_ext = config_['datasets_info']['indexed_dataset']['extension']\n",
    "        data_path = config_['datasets_info']['indexed_dataset']['path']\n",
    "        \n",
    "        paralelism = int(config_['datasets_info']['indexed_dataset']['default_paralelism'])\n",
    "        \n",
    "        prefix_sl = \"StorageLevel.\"\n",
    "        storage_level = config['datasets_info']['indexed_dataset']['storage_level']\n",
    "        \n",
    "        # test the extension of the dataset to properly read it\n",
    "        if data_ext == 'csv':\n",
    "            indexed_dataset = spark.read.csv(data_path, header=True).repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "        elif data_ext == 'parquet':\n",
    "            indexed_dataset = spark.read.parquet(data_path).repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "        else:\n",
    "            print(\"Please make sure the extension for this dataset is set as 'csv' or 'parquet'\")\n",
    "    \n",
    "        # # indexing, at last\n",
    "        index_dataframe(indexed_dataset, index_name)\n",
    "        print(\"[CIDACS-RL] indexing on, it took {} secs\".format(time.time()-start_))\n",
    "    \n",
    "    # getting the auxiliary variables\n",
    "    data_ext = config_['datasets_info']['tolink_dataset']['extension']\n",
    "    data_path = config_['datasets_info']['tolink_dataset']['path']\n",
    "    \n",
    "    paralelism = int(config_['datasets_info']['tolink_dataset']['default_paralelism'])\n",
    "    \n",
    "    prefix_sl = \"StorageLevel.\"\n",
    "    storage_level = config['datasets_info']['tolink_dataset']['storage_level']\n",
    "\n",
    "    # test the extension of the dataset to properly read it\n",
    "    if data_ext == 'csv':\n",
    "        tolink_dataset = spark.read.csv(data_path, header=True).repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "    elif data_ext == 'parquet':\n",
    "        tolink_dataset = spark.read.parquet(data_path).repartition(paralelism).persist(eval(prefix_sl+storage_level))\n",
    "    else:\n",
    "        print(\"Please make sure the extension for this dataset is set as 'csv' or 'parquet'\")\n",
    "    \n",
    "    tolink_dataset = cidacs_rl_exact_phase(tolink_dataset)\n",
    "    \n",
    "    tolink_dataset = cidacs_rl_non_exact_phase(tolink_dataset)\n",
    "    \n",
    "    tolink_dataset = tolink_dataset.withColumn('final_cidacs_rl_score', \n",
    "                                               F.when(F.col('linked_from') == 'exact_match', F.col('sim_best_candidate_exact'))\\\n",
    "                                                .otherwise(F.col('sim_best_candidate_non_exact')))\n",
    "    \n",
    "    tolink_dataset = tolink_dataset.withColumn('final_cidacs_rl_id', \n",
    "                                               F.when(F.col('linked_from') == 'exact_match', F.col('best_candidate_exact'))\\\n",
    "                                                .otherwise(F.col('best_candidate_non_exact')))\n",
    "    \n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    print(\"[CIDACS-RL] finished at {}\".format(dt_string))\n",
    "    print(\"[CIDACS-RL] total time elapsed: {} secs\".format(time.time()-start))\n",
    "    return tolink_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running CIDACS-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'config.json'\n",
    "f = open(config_file)\n",
    "config = json.load(f)\n",
    "\n",
    "# broadcasting config\n",
    "config_bc = sc.broadcast(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_data': 'no',\n",
       " 'es_index_name': 'fd-cidacs-rl',\n",
       " 'es_connect_string': 'http://localhost:9200',\n",
       " 'query_size': 50,\n",
       " 'cutoff_exact_match': '0.95',\n",
       " 'null_value': '99',\n",
       " 'temp_dir': '../temp_dataframe/',\n",
       " 'debug': 'false',\n",
       " 'write_checkpoint': 'false',\n",
       " 'datasets_info': {'indexed_dataset': {'path': '../data/sinthetic-dataset-A.parquet',\n",
       "   'extension': 'parquet',\n",
       "   'columns': ['id_cidacs_a', 'nome_a', 'nome_mae_a', 'dt_nasc_a', 'sexo_a'],\n",
       "   'id_column_name': 'id_cidacs_a',\n",
       "   'storage_level': 'MEMORY_ONLY',\n",
       "   'default_paralelism': '16'},\n",
       "  'tolink_dataset': {'path': '../data/sinthetic-datasets-b-1000.parquet',\n",
       "   'extension': 'parquet',\n",
       "   'columns': ['id_cidacs_b', 'nome_b', 'nome_mae_b', 'dt_nasc_b', 'sexo_b'],\n",
       "   'id_column_name': 'id_cidacs_b',\n",
       "   'storage_level': 'MEMORY_ONLY',\n",
       "   'default_paralelism': '16'},\n",
       "  'result_dataset': {'path': '../result/'}},\n",
       " 'comparisons': {'name': {'indexed_col': 'nome_a',\n",
       "   'tolink_col': 'nome_b',\n",
       "   'must_match': 'true',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'true',\n",
       "   'boost': '3.0',\n",
       "   'query_type': 'match',\n",
       "   'similarity': 'jaro_winkler',\n",
       "   'weight': 5.0,\n",
       "   'penalty': 0.02},\n",
       "  'mothers_name': {'indexed_col': 'nome_mae_a',\n",
       "   'tolink_col': 'nome_mae_b',\n",
       "   'must_match': 'true',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'true',\n",
       "   'boost': '2.0',\n",
       "   'query_type': 'match',\n",
       "   'similarity': 'jaro_winkler',\n",
       "   'weight': 5.0,\n",
       "   'penalty': 0.02},\n",
       "  'birthdate': {'indexed_col': 'dt_nasc_a',\n",
       "   'tolink_col': 'dt_nasc_b',\n",
       "   'must_match': 'false',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'false',\n",
       "   'boost': '',\n",
       "   'query_type': 'term',\n",
       "   'similarity': 'hamming',\n",
       "   'weight': 1.0,\n",
       "   'penalty': 0.02},\n",
       "  'sex': {'indexed_col': 'sexo_a',\n",
       "   'tolink_col': 'sexo_b',\n",
       "   'must_match': 'true',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'false',\n",
       "   'boost': '',\n",
       "   'query_type': 'term',\n",
       "   'similarity': 'overlap',\n",
       "   'weight': 3.0,\n",
       "   'penalty': 0.02}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_1 = spark.read.parquet('../data/sinthetic-dataset-A.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CIDACS-RL] starting at 12/08/2025 19:36:51\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o54.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 172.18.0.3, executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:376)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:444)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:490)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:484)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:75)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.FileNotFoundException: File file:/root/data/sinthetic-datasets-b-1000.parquet/part-00000-1c7b70cc-9ae0-4b61-8dff-fb2f23c2279b-c000.snappy.parquet does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:454)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:65)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:498)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:451)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:373)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:494)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:107)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:163)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:198)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:195)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:755)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:376)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:444)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:490)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:484)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:75)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: File file:/root/data/sinthetic-datasets-b-1000.parquet/part-00000-1c7b70cc-9ae0-4b61-8dff-fb2f23c2279b-c000.snappy.parquet does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:454)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:65)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:498)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:451)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:373)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-414278c820ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlinked_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcidacsrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9c87f9a0c3f9>\u001b[0m in \u001b[0;36mcidacsrl\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mtolink_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparalelism\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_sl\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstorage_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdata_ext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'parquet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mtolink_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparalelism\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_sl\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstorage_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please make sure the extension for this dataset is set as 'csv' or 'parquet'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    351\u001b[0m         self._set_opts(mergeSchema=mergeSchema, pathGlobFilter=pathGlobFilter,\n\u001b[1;32m    352\u001b[0m                        recursiveFileLookup=recursiveFileLookup)\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o54.parquet.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, 172.18.0.3, executor 1): org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:376)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:444)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:490)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:484)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:75)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.FileNotFoundException: File file:/root/data/sinthetic-datasets-b-1000.parquet/part-00000-1c7b70cc-9ae0-4b61-8dff-fb2f23c2279b-c000.snappy.parquet does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:454)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:65)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:498)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:451)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:373)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:69)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:494)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:107)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:163)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:198)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:195)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:755)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:376)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:444)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:490)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:484)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:75)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.io.FileNotFoundException: File file:/root/data/sinthetic-datasets-b-1000.parquet/part-00000-1c7b70cc-9ae0-4b61-8dff-fb2f23c2279b-c000.snappy.parquet does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:454)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:146)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)\n\tat org.apache.parquet.hadoop.util.HadoopInputFile.newStream(HadoopInputFile.java:65)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:498)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:451)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:373)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n"
     ]
    }
   ],
   "source": [
    "linked_data = cidacsrl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cidacs_b</th>\n",
       "      <th>nome_b</th>\n",
       "      <th>nome_mae_b</th>\n",
       "      <th>dt_nasc_b</th>\n",
       "      <th>sexo_b</th>\n",
       "      <th>vars</th>\n",
       "      <th>exact_queries</th>\n",
       "      <th>best_candidate_exact</th>\n",
       "      <th>sim_best_candidate_exact</th>\n",
       "      <th>similarity_exact_candidates</th>\n",
       "      <th>linked_from</th>\n",
       "      <th>non_exact_queries</th>\n",
       "      <th>best_candidate_non_exact</th>\n",
       "      <th>sim_best_candidate_non_exact</th>\n",
       "      <th>similarity_non_exact_candidates</th>\n",
       "      <th>final_cidacs_rl_score</th>\n",
       "      <th>final_cidacs_rl_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7314</td>\n",
       "      <td>IRLANA CORREIA</td>\n",
       "      <td>IRLANA CORREIA</td>\n",
       "      <td>20090417</td>\n",
       "      <td>1</td>\n",
       "      <td>[7314, IRLANA CORREIA, IRLANA CORREIA, 2009041...</td>\n",
       "      <td>{ \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"IRL...</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>non_exact_match</td>\n",
       "      <td>{ \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...</td>\n",
       "      <td>722901</td>\n",
       "      <td>0.861511</td>\n",
       "      <td>{106761=0.7999843615607164, 425580=0.850456091...</td>\n",
       "      <td>0.861511</td>\n",
       "      <td>722901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4326</td>\n",
       "      <td>LIDIA BELLONIA BARBOSA</td>\n",
       "      <td>LIDIA BELLONIA BARBOSA</td>\n",
       "      <td>20080822</td>\n",
       "      <td>1</td>\n",
       "      <td>[4326, LIDIA BELLONIA BARBOSA, LIDIA BELLONIA ...</td>\n",
       "      <td>{ \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"LID...</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>non_exact_match</td>\n",
       "      <td>{ \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...</td>\n",
       "      <td>338244</td>\n",
       "      <td>0.865734</td>\n",
       "      <td>{516678=0.8331529581529581, 886362=0.772524350...</td>\n",
       "      <td>0.865734</td>\n",
       "      <td>338244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18266</td>\n",
       "      <td>RAPHAEL MIQUEIAS NASCIMENTO</td>\n",
       "      <td>CAROLINA SILVA</td>\n",
       "      <td>20110117</td>\n",
       "      <td>2</td>\n",
       "      <td>[18266, RAPHAEL MIQUEIAS NASCIMENTO, CAROLINA ...</td>\n",
       "      <td>{ \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"RAP...</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>non_exact_match</td>\n",
       "      <td>{ \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...</td>\n",
       "      <td>18266</td>\n",
       "      <td>0.747199</td>\n",
       "      <td>{918320=0.6190809657196211, 860252=0.631185428...</td>\n",
       "      <td>0.747199</td>\n",
       "      <td>18266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_cidacs_b                       nome_b              nome_mae_b dt_nasc_b  \\\n",
       "0        7314               IRLANA CORREIA          IRLANA CORREIA  20090417   \n",
       "1        4326       LIDIA BELLONIA BARBOSA  LIDIA BELLONIA BARBOSA  20080822   \n",
       "2       18266  RAPHAEL MIQUEIAS NASCIMENTO          CAROLINA SILVA  20110117   \n",
       "\n",
       "  sexo_b                                               vars  \\\n",
       "0      1  [7314, IRLANA CORREIA, IRLANA CORREIA, 2009041...   \n",
       "1      1  [4326, LIDIA BELLONIA BARBOSA, LIDIA BELLONIA ...   \n",
       "2      2  [18266, RAPHAEL MIQUEIAS NASCIMENTO, CAROLINA ...   \n",
       "\n",
       "                                       exact_queries best_candidate_exact  \\\n",
       "0  { \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"IRL...                 null   \n",
       "1  { \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"LID...                 null   \n",
       "2  { \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"RAP...                 null   \n",
       "\n",
       "   sim_best_candidate_exact similarity_exact_candidates      linked_from  \\\n",
       "0                       NaN                        null  non_exact_match   \n",
       "1                       NaN                        null  non_exact_match   \n",
       "2                       NaN                        null  non_exact_match   \n",
       "\n",
       "                                   non_exact_queries best_candidate_non_exact  \\\n",
       "0  { \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...                   722901   \n",
       "1  { \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...                   338244   \n",
       "2  { \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...                    18266   \n",
       "\n",
       "   sim_best_candidate_non_exact  \\\n",
       "0                      0.861511   \n",
       "1                      0.865734   \n",
       "2                      0.747199   \n",
       "\n",
       "                     similarity_non_exact_candidates  final_cidacs_rl_score  \\\n",
       "0  {106761=0.7999843615607164, 425580=0.850456091...               0.861511   \n",
       "1  {516678=0.8331529581529581, 886362=0.772524350...               0.865734   \n",
       "2  {918320=0.6190809657196211, 860252=0.631185428...               0.747199   \n",
       "\n",
       "  final_cidacs_rl_id  \n",
       "0             722901  \n",
       "1             338244  \n",
       "2              18266  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linked_data.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CIDACS-RL] starting at 26/01/2022 20:56:21\n",
    "# \t[CIDACS-RL] time for exact phase: 38.057870864868164 secs\n",
    "# \t[CIDACS-RL] time for non-exact phase: 45.42489194869995 secs\n",
    "# [CIDACS-RL] finished at 26/01/2022 20:57:49\n",
    "# [CIDACS-RL] total time elapsed: 88.37369227409363 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cidacs_b</th>\n",
       "      <th>nome_b</th>\n",
       "      <th>nome_mae_b</th>\n",
       "      <th>dt_nasc_b</th>\n",
       "      <th>sexo_b</th>\n",
       "      <th>vars</th>\n",
       "      <th>exact_queries</th>\n",
       "      <th>best_candidate_exact</th>\n",
       "      <th>sim_best_candidate_exact</th>\n",
       "      <th>similarity_exact_candidates</th>\n",
       "      <th>linked_from</th>\n",
       "      <th>non_exact_queries</th>\n",
       "      <th>best_candidate_non_exact</th>\n",
       "      <th>sim_best_candidate_non_exact</th>\n",
       "      <th>similarity_non_exact_candidates</th>\n",
       "      <th>final_cidacs_rl_score</th>\n",
       "      <th>final_cidacs_rl_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7314</td>\n",
       "      <td>IRLANA CORREIA</td>\n",
       "      <td>IRLANA CORREIA</td>\n",
       "      <td>20090417</td>\n",
       "      <td>1</td>\n",
       "      <td>[7314, IRLANA CORREIA, IRLANA CORREIA, 2009041...</td>\n",
       "      <td>{ \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"IRL...</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>non_exact_match</td>\n",
       "      <td>{ \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...</td>\n",
       "      <td>722901</td>\n",
       "      <td>0.861511</td>\n",
       "      <td>{106761=0.7999843615607164, 425580=0.850456091...</td>\n",
       "      <td>0.861511</td>\n",
       "      <td>722901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4326</td>\n",
       "      <td>LIDIA BELLONIA BARBOSA</td>\n",
       "      <td>LIDIA BELLONIA BARBOSA</td>\n",
       "      <td>20080822</td>\n",
       "      <td>1</td>\n",
       "      <td>[4326, LIDIA BELLONIA BARBOSA, LIDIA BELLONIA ...</td>\n",
       "      <td>{ \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"LID...</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>non_exact_match</td>\n",
       "      <td>{ \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...</td>\n",
       "      <td>338244</td>\n",
       "      <td>0.865734</td>\n",
       "      <td>{516678=0.8331529581529581, 886362=0.772524350...</td>\n",
       "      <td>0.865734</td>\n",
       "      <td>338244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18266</td>\n",
       "      <td>RAPHAEL MIQUEIAS NASCIMENTO</td>\n",
       "      <td>CAROLINA SILVA</td>\n",
       "      <td>20110117</td>\n",
       "      <td>2</td>\n",
       "      <td>[18266, RAPHAEL MIQUEIAS NASCIMENTO, CAROLINA ...</td>\n",
       "      <td>{ \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"RAP...</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>non_exact_match</td>\n",
       "      <td>{ \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...</td>\n",
       "      <td>18266</td>\n",
       "      <td>0.747199</td>\n",
       "      <td>{918320=0.6190809657196211, 860252=0.631185428...</td>\n",
       "      <td>0.747199</td>\n",
       "      <td>18266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_cidacs_b                       nome_b              nome_mae_b dt_nasc_b  \\\n",
       "0        7314               IRLANA CORREIA          IRLANA CORREIA  20090417   \n",
       "1        4326       LIDIA BELLONIA BARBOSA  LIDIA BELLONIA BARBOSA  20080822   \n",
       "2       18266  RAPHAEL MIQUEIAS NASCIMENTO          CAROLINA SILVA  20110117   \n",
       "\n",
       "  sexo_b                                               vars  \\\n",
       "0      1  [7314, IRLANA CORREIA, IRLANA CORREIA, 2009041...   \n",
       "1      1  [4326, LIDIA BELLONIA BARBOSA, LIDIA BELLONIA ...   \n",
       "2      2  [18266, RAPHAEL MIQUEIAS NASCIMENTO, CAROLINA ...   \n",
       "\n",
       "                                       exact_queries best_candidate_exact  \\\n",
       "0  { \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"IRL...                 null   \n",
       "1  { \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"LID...                 null   \n",
       "2  { \"bool\": { \"must\": [ {\"match\": {\"nome_a\":\"RAP...                 null   \n",
       "\n",
       "   sim_best_candidate_exact similarity_exact_candidates      linked_from  \\\n",
       "0                       NaN                        null  non_exact_match   \n",
       "1                       NaN                        null  non_exact_match   \n",
       "2                       NaN                        null  non_exact_match   \n",
       "\n",
       "                                   non_exact_queries best_candidate_non_exact  \\\n",
       "0  { \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...                   722901   \n",
       "1  { \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...                   338244   \n",
       "2  { \"bool\": { \"should\": [ {\"match\": {\"nome_a\": {...                    18266   \n",
       "\n",
       "   sim_best_candidate_non_exact  \\\n",
       "0                      0.861511   \n",
       "1                      0.865734   \n",
       "2                      0.747199   \n",
       "\n",
       "                     similarity_non_exact_candidates  final_cidacs_rl_score  \\\n",
       "0  {106761=0.7999843615607164, 425580=0.850456091...               0.861511   \n",
       "1  {516678=0.8331529581529581, 886362=0.772524350...               0.865734   \n",
       "2  {918320=0.6190809657196211, 860252=0.631185428...               0.747199   \n",
       "\n",
       "  final_cidacs_rl_id  \n",
       "0             722901  \n",
       "1             338244  \n",
       "2              18266  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linked_data.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total de execuo: 95.93091297149658 secs\n"
     ]
    }
   ],
   "source": [
    "print(\"Tempo total de execuo: {} secs\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tempo total de execuo: 105.01147866249084 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|    linked_from|count|\n",
      "+---------------+-----+\n",
      "|non_exact_match|  458|\n",
      "|    exact_match|  542|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linked_data.select('linked_from').groupBy('linked_from').count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
