{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import jellyfish\n",
    "from elasticsearch import Elasticsearch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option(\"display.max_columns\", 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_data': 'yes',\n",
       " 'index_name': 'fd-cidacs-rl',\n",
       " 'query_size': 50,\n",
       " 'datasets_info': {'indexed_dataset': {'path': '../0_global_data/fd-cidacs-rl/sinthetic-dataset-A.parquet',\n",
       "   'extension': 'parquet',\n",
       "   'columns': ['id_cidacs_a', 'nome_a', 'nome_mae_a', 'dt_nasc_a', 'sexo_a']},\n",
       "  'tolink_dataset': {'path': '../0_global_data/fd-cidacs-rl/sinthetic-dataset-B.csv',\n",
       "   'extension': 'csv',\n",
       "   'columns': ['id_cidacs_b', 'nome_b', 'nome_mae_b', 'dt_nasc_b', 'sexo_b']},\n",
       "  'result_dataset': {'path': '../0_global_data/result/'}},\n",
       " 'comparisons': {'nome_a': {'compare_to': 'nome_b',\n",
       "   'exact_match': 'true',\n",
       "   'fuzzy_match': 'true',\n",
       "   'query_type': 'match',\n",
       "   'similarity': 'jaro_wikler',\n",
       "   'weight': 1.0,\n",
       "   'penalty': 0.02},\n",
       "  'nome_mae_a': {'compare_to': 'nome_mae_b',\n",
       "   'exact_match': 'true',\n",
       "   'fuzzy_match': 'true',\n",
       "   'query_type': 'match',\n",
       "   'similarity': 'jaro_wikler',\n",
       "   'weight': 1.0,\n",
       "   'penalty': 0.02},\n",
       "  'dt_nasc_a': {'compare_to': 'dt_nasc_b',\n",
       "   'exact_match': 'false',\n",
       "   'fuzzy_match': 'true',\n",
       "   'query_type': 'term',\n",
       "   'similarity': 'hamming',\n",
       "   'weight': 1.0,\n",
       "   'penalty': 0.02},\n",
       "  'sexo_a': {'compare_to': 'sexo_b',\n",
       "   'exact_match': 'true',\n",
       "   'fuzzy_match': 'true',\n",
       "   'query_type': 'term',\n",
       "   'similarity': 'overlap',\n",
       "   'weight': 1.0,\n",
       "   'penalty': 0.02}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('config.txt')\n",
    "config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading prepocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the auxiliary variables\n",
    "data_ext = config['datasets_info']['indexed_dataset']['extension']\n",
    "data_path = config['datasets_info']['indexed_dataset']['path']\n",
    "\n",
    "# test the extension of the dataset to properly read it\n",
    "if data_ext == 'csv':\n",
    "    indexed_dataset = spark.read.csv(data_path, header=True)\n",
    "elif data_ext == 'parquet':\n",
    "    indexed_dataset = spark.read.parquet(data_path)\n",
    "else:\n",
    "    print(\"Please make sure the extension for this dataset is set as 'csv' or 'parquet'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating sinthetic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supress_last_name(col):\n",
    "    col = str(col)\n",
    "    return ' '.join(col.split(' ')[:-1])\n",
    "udf_supress_last_name = F.udf(supress_last_name, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '../0_global_data/fd-cidacs-rl/sinthetic-datasets-b/sinthetic-datasets-b-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_sizes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_df': 54, 'gray_df': 20, 'false_df': 10141, 'accum_size': 10215, 'final_size': 100}\n",
      "{'true_df': 250, 'gray_df': 109, 'false_df': 10167, 'accum_size': 10526, 'final_size': 500}\n",
      "{'true_df': 515, 'gray_df': 196, 'false_df': 10229, 'accum_size': 10940, 'final_size': 1000}\n",
      "{'true_df': 2460, 'gray_df': 1019, 'false_df': 11437, 'accum_size': 14916, 'final_size': 5000}\n",
      "{'true_df': 5170, 'gray_df': 1936, 'false_df': 13008, 'accum_size': 20114, 'final_size': 10000}\n",
      "{'true_df': 24734, 'gray_df': 9868, 'false_df': 25110, 'accum_size': 59712, 'final_size': 50000}\n",
      "{'true_df': 49889, 'gray_df': 20133, 'false_df': 40296, 'accum_size': 110318, 'final_size': 100000}\n",
      "{'true_df': 250628, 'gray_df': 100149, 'false_df': 160057, 'accum_size': 510834, 'final_size': 500000}\n",
      "{'true_df': 500197, 'gray_df': 200196, 'false_df': 309491, 'accum_size': 1009884, 'final_size': 1000000}\n"
     ]
    }
   ],
   "source": [
    "for size in sizes:\n",
    "    accum_size = 0\n",
    "    map_sizes[str(size)] = {}\n",
    "    # setting the proportions of exact true matches, gray area and false matches\n",
    "    # ~50% of true matches\n",
    "    # ~20% of gray area (record with last name supression and wrong information on sex)\n",
    "    # ~30% of false matches\n",
    "    n_true_m = (size/100)*50\n",
    "    n_gray_m = (size/100)*20\n",
    "    n_false_m = (size/100)*30\n",
    "    \n",
    "    # using literal numbers to estimate the right proportion of sample\n",
    "    p_true_m = n_true_m/1000000\n",
    "    p_gray_m = n_gray_m/1000000\n",
    "    p_false_m = (n_false_m/1000000)+ 0.01\n",
    "    \n",
    "    # getting sample of exact true\n",
    "    true_df = indexed_dataset.sample(p_true_m)\n",
    "    count = true_df.count()\n",
    "    accum_size += count\n",
    "    map_sizes[str(size)]['true_df'] = count\n",
    "    \n",
    "    # getting sample of gray area\n",
    "    gray_df = indexed_dataset.sample(p_gray_m)\n",
    "    count = gray_df.count()\n",
    "    accum_size += count\n",
    "    map_sizes[str(size)]['gray_df'] = count\n",
    "    \n",
    "    # getting sample of false matches\n",
    "    false_df = indexed_dataset.sample(p_false_m)\n",
    "    count = false_df.count()\n",
    "    accum_size += count\n",
    "    map_sizes[str(size)]['false_df'] = count\n",
    "    \n",
    "    # recording the total size of resulting datasets\n",
    "    map_sizes[str(size)]['accum_size'] = accum_size\n",
    "    \n",
    "    # suppressing last name and changing sex info for gray area records\n",
    "    gray_df = gray_df.withColumn('nome_a', udf_supress_last_name(F.col('nome_a')))\n",
    "    gray_df = gray_df.withColumn('nome_mae_a', udf_supress_last_name(F.col('nome_mae_a')))\n",
    "    gray_df = gray_df.withColumn('sexo_a', F.when(F.col('sexo_a') == 1, 2).otherwise(1))\n",
    "    \n",
    "    # union\n",
    "    df = true_df.union(gray_df).union(false_df).limit(size)\n",
    "    count = df.count()\n",
    "    map_sizes[str(size)]['final_size'] = count\n",
    "    \n",
    "    # changing names\n",
    "    names_dict = {'id_cidacs_a': 'id_cidacs_b', \n",
    "                  'nome_a': 'nome_b', \n",
    "                  'nome_mae_a': 'nome_mae_b', \n",
    "                  'dt_nasc_a': 'dt_nasc_b', \n",
    "                  'sexo_a': 'sexo_b'}\n",
    "    for col in names_dict.keys():\n",
    "        df = df.withColumnRenamed(col, names_dict[col])\n",
    "    \n",
    "    # writing data\n",
    "    df.write.parquet(prefix+str(size)+'.parquet', mode='overwrite')\n",
    "    print(map_sizes[str(size)])\n",
    "\n",
    "# map_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cidacs_b</th>\n",
       "      <th>nome_b</th>\n",
       "      <th>nome_mae_b</th>\n",
       "      <th>dt_nasc_b</th>\n",
       "      <th>sexo_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>THAYSLANE VITORIA DA ROCHA SOARES</td>\n",
       "      <td>JANAYNA MARTINS DA SILVA</td>\n",
       "      <td>2007-10-12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>658</td>\n",
       "      <td>SARA DUARTE DE FRANCA</td>\n",
       "      <td>JOCILENE SOUZA DOS SANTOS</td>\n",
       "      <td>2007-08-22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1068</td>\n",
       "      <td>MATHEUS MONTEIRO SANTOS</td>\n",
       "      <td>CARINA DA SILVA CRISTINO</td>\n",
       "      <td>2007-05-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1087</td>\n",
       "      <td>FELIPE LIDUINO RODRIGUES</td>\n",
       "      <td>EDNALVA DUARTE</td>\n",
       "      <td>2008-04-30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1155</td>\n",
       "      <td>MATEUS VIEIRA DOS SANTOS JUNIOR</td>\n",
       "      <td>NIVEA GRAZIELLA SILVA DE AZEVEDO</td>\n",
       "      <td>2006-09-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_cidacs_b                             nome_b  \\\n",
       "0          92  THAYSLANE VITORIA DA ROCHA SOARES   \n",
       "1         658              SARA DUARTE DE FRANCA   \n",
       "2        1068            MATHEUS MONTEIRO SANTOS   \n",
       "3        1087           FELIPE LIDUINO RODRIGUES   \n",
       "4        1155    MATEUS VIEIRA DOS SANTOS JUNIOR   \n",
       "\n",
       "                         nome_mae_b   dt_nasc_b  sexo_b  \n",
       "0          JANAYNA MARTINS DA SILVA  2007-10-12       2  \n",
       "1         JOCILENE SOUZA DOS SANTOS  2007-08-22       2  \n",
       "2          CARINA DA SILVA CRISTINO  2007-05-22       1  \n",
       "3                    EDNALVA DUARTE  2008-04-30       1  \n",
       "4  NIVEA GRAZIELLA SILVA DE AZEVEDO  2006-09-29       1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing one dataset\n",
    "spark.read.parquet('../0_global_data/fd-cidacs-rl/sinthetic-datasets-b/sinthetic-datasets-b-10000.parquet/').limit(5).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
