{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import jellyfish\n",
    "from elasticsearch import Elasticsearch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option(\"display.max_columns\", 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_data': 'yes',\n",
       " 'es_index_name': 'fd-cidacs-rl',\n",
       " 'es_connect_string': 'http://localhost:9200',\n",
       " 'query_size': 50,\n",
       " 'cutoff_exact_match': '0.95',\n",
       " 'null_value': '99',\n",
       " 'temp_dir': '../0_global_data/fd-cidacs-rl/temp_dataframe/',\n",
       " 'datasets_info': {'indexed_dataset': {'path': '../0_global_data/fd-cidacs-rl/sinthetic-dataset-A.parquet',\n",
       "   'extension': 'parquet',\n",
       "   'columns': ['id_cidacs_a', 'nome_a', 'nome_mae_a', 'dt_nasc_a', 'sexo_a'],\n",
       "   'id_column_name': 'id_cidacs_a'},\n",
       "  'tolink_dataset': {'path': '../0_global_data/fd-cidacs-rl/sinthetic-datasets-b/sinthetic-datasets-b-1000.parquet',\n",
       "   'extension': 'parquet',\n",
       "   'columns': ['id_cidacs_b', 'nome_b', 'nome_mae_b', 'dt_nasc_b', 'sexo_b'],\n",
       "   'id_column_name': 'id_cidacs_b'},\n",
       "  'result_dataset': {'path': '../0_global_data/result/'}},\n",
       " 'comparisons': {'name': {'indexed_col': 'nome_a',\n",
       "   'tolink_col': 'nome_b',\n",
       "   'must_match': 'true',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'true',\n",
       "   'boost': '3.0',\n",
       "   'query_type': 'match',\n",
       "   'similarity': 'jaro_winkler',\n",
       "   'weight': 5.0,\n",
       "   'penalty': 0.02},\n",
       "  'mothers_name': {'indexed_col': 'nome_mae_a',\n",
       "   'tolink_col': 'nome_mae_b',\n",
       "   'must_match': 'true',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'true',\n",
       "   'boost': '2.0',\n",
       "   'query_type': 'match',\n",
       "   'similarity': 'jaro_winkler',\n",
       "   'weight': 5.0,\n",
       "   'penalty': 0.02},\n",
       "  'birthdate': {'indexed_col': 'dt_nasc_a',\n",
       "   'tolink_col': 'dt_nasc_b',\n",
       "   'must_match': 'false',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'false',\n",
       "   'boost': '',\n",
       "   'query_type': 'term',\n",
       "   'similarity': 'hamming',\n",
       "   'weight': 1.0,\n",
       "   'penalty': 0.02},\n",
       "  'sex': {'indexed_col': 'sexo_a',\n",
       "   'tolink_col': 'sexo_b',\n",
       "   'must_match': 'true',\n",
       "   'should_match': 'true',\n",
       "   'is_fuzzy': 'false',\n",
       "   'boost': '',\n",
       "   'query_type': 'term',\n",
       "   'similarity': 'overlap',\n",
       "   'weight': 3.0,\n",
       "   'penalty': 0.02}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('config.txt')\n",
    "config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading prepocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the auxiliary variables\n",
    "data_ext = config['datasets_info']['indexed_dataset']['extension']\n",
    "data_path = config['datasets_info']['indexed_dataset']['path']\n",
    "\n",
    "# test the extension of the dataset to properly read it\n",
    "if data_ext == 'csv':\n",
    "    indexed_dataset = spark.read.csv(data_path, header=True)\n",
    "elif data_ext == 'parquet':\n",
    "    indexed_dataset = spark.read.parquet(data_path)\n",
    "else:\n",
    "    print(\"Please make sure the extension for this dataset is set as 'csv' or 'parquet'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating sinthetic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supress_last_name(col):\n",
    "    col = str(col)\n",
    "    return ' '.join(col.split(' ')[:-1])\n",
    "udf_supress_last_name = F.udf(supress_last_name, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '../0_global_data/fd-cidacs-rl/sinthetic-datasets-b/sinthetic-datasets-b-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_sizes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true_df': 53, 'gray_df': 20, 'false_df': 10110, 'accum_size': 10183, 'final_size': 100}\n",
      "{'true_df': 257, 'gray_df': 104, 'false_df': 10166, 'accum_size': 10527, 'final_size': 500}\n",
      "{'true_df': 502, 'gray_df': 199, 'false_df': 10319, 'accum_size': 11020, 'final_size': 1000}\n",
      "{'true_df': 2504, 'gray_df': 970, 'false_df': 11295, 'accum_size': 14769, 'final_size': 5000}\n",
      "{'true_df': 5001, 'gray_df': 2011, 'false_df': 13106, 'accum_size': 20118, 'final_size': 10000}\n",
      "{'true_df': 25185, 'gray_df': 9949, 'false_df': 25143, 'accum_size': 60277, 'final_size': 50000}\n",
      "{'true_df': 50252, 'gray_df': 20282, 'false_df': 40013, 'accum_size': 110547, 'final_size': 100000}\n",
      "{'true_df': 249809, 'gray_df': 99845, 'false_df': 159722, 'accum_size': 509376, 'final_size': 500000}\n",
      "{'true_df': 499805, 'gray_df': 200412, 'false_df': 309901, 'accum_size': 1010118, 'final_size': 1000000}\n"
     ]
    }
   ],
   "source": [
    "for size in sizes:\n",
    "    accum_size = 0\n",
    "    map_sizes[str(size)] = {}\n",
    "    # setting the proportions of exact true matches, gray area and false matches\n",
    "    # ~50% of true matches\n",
    "    # ~20% of gray area (record with last name supression and wrong information on sex)\n",
    "    # ~30% of false matches\n",
    "    n_true_m = (size/100)*50\n",
    "    n_gray_m = (size/100)*20\n",
    "    n_false_m = (size/100)*30\n",
    "    \n",
    "    # using literal numbers to estimate the right proportion of sample\n",
    "    p_true_m = n_true_m/1000000\n",
    "    p_gray_m = n_gray_m/1000000\n",
    "    p_false_m = (n_false_m/1000000)+ 0.01\n",
    "    \n",
    "    # getting sample of exact true\n",
    "    true_df = indexed_dataset.sample(p_true_m)\n",
    "    count = true_df.count()\n",
    "    accum_size += count\n",
    "    map_sizes[str(size)]['true_df'] = count\n",
    "    \n",
    "    # getting sample of gray area\n",
    "    gray_df = indexed_dataset.sample(p_gray_m)\n",
    "    count = gray_df.count()\n",
    "    accum_size += count\n",
    "    map_sizes[str(size)]['gray_df'] = count\n",
    "    \n",
    "    # getting sample of false matches\n",
    "    false_df = indexed_dataset.sample(p_false_m)\n",
    "    count = false_df.count()\n",
    "    accum_size += count\n",
    "    map_sizes[str(size)]['false_df'] = count\n",
    "    \n",
    "    # recording the total size of resulting datasets\n",
    "    map_sizes[str(size)]['accum_size'] = accum_size\n",
    "    \n",
    "    # suppressing last name and changing sex info for gray area records\n",
    "    gray_df = gray_df.withColumn('nome_a', udf_supress_last_name(F.col('nome_a')))\n",
    "    gray_df = gray_df.withColumn('nome_mae_a', udf_supress_last_name(F.col('nome_mae_a')))\n",
    "    gray_df = gray_df.withColumn('sexo_a', F.when(F.col('sexo_a') == 1, 2).otherwise(1))\n",
    "    \n",
    "    \n",
    "    # messing with name for false matches\n",
    "    false_df = false_df.withColumn('nome_a', F.col('nome_mae_a'))\n",
    "    \n",
    "    # union\n",
    "    df = true_df.union(gray_df).union(false_df).limit(size)\n",
    "    count = df.count()\n",
    "    map_sizes[str(size)]['final_size'] = count\n",
    "    \n",
    "    # changing names\n",
    "    names_dict = {'id_cidacs_a': 'id_cidacs_b', \n",
    "                  'nome_a': 'nome_b', \n",
    "                  'nome_mae_a': 'nome_mae_b', \n",
    "                  'dt_nasc_a': 'dt_nasc_b', \n",
    "                  'sexo_a': 'sexo_b'}\n",
    "    for col in names_dict.keys():\n",
    "        df = df.withColumnRenamed(col, names_dict[col])\n",
    "    \n",
    "    # writing data\n",
    "    df.write.parquet(prefix+str(size)+'.parquet', mode='overwrite')\n",
    "    print(map_sizes[str(size)])\n",
    "\n",
    "# map_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_cidacs_b</th>\n",
       "      <th>nome_b</th>\n",
       "      <th>nome_mae_b</th>\n",
       "      <th>dt_nasc_b</th>\n",
       "      <th>sexo_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>388</td>\n",
       "      <td>VICTOR LUCAS SANTOS DE ALMEIDA</td>\n",
       "      <td>EDINEUZA SANTOS PEDRO</td>\n",
       "      <td>2007-10-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>755</td>\n",
       "      <td>MURILO ESPOLADOR CORDEIRO</td>\n",
       "      <td>JAQUELINE DE SOUZA RODRIGUES</td>\n",
       "      <td>2008-02-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1201</td>\n",
       "      <td>ICARO GABRIEL DA SILVA FONSECA</td>\n",
       "      <td>ANA PAULA OSTINU</td>\n",
       "      <td>2007-08-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1443</td>\n",
       "      <td>JEAN LUCAS AYRES</td>\n",
       "      <td>DANIELLE DE SOUZA MENEZES</td>\n",
       "      <td>2008-04-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1895</td>\n",
       "      <td>ICARO MANUEL DE OLIVEIRA CHAVIER</td>\n",
       "      <td>SOLANGE DA SILVA</td>\n",
       "      <td>2008-10-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_cidacs_b                            nome_b                    nome_mae_b  \\\n",
       "0         388    VICTOR LUCAS SANTOS DE ALMEIDA         EDINEUZA SANTOS PEDRO   \n",
       "1         755         MURILO ESPOLADOR CORDEIRO  JAQUELINE DE SOUZA RODRIGUES   \n",
       "2        1201    ICARO GABRIEL DA SILVA FONSECA              ANA PAULA OSTINU   \n",
       "3        1443                  JEAN LUCAS AYRES     DANIELLE DE SOUZA MENEZES   \n",
       "4        1895  ICARO MANUEL DE OLIVEIRA CHAVIER              SOLANGE DA SILVA   \n",
       "\n",
       "    dt_nasc_b  sexo_b  \n",
       "0  2007-10-06       1  \n",
       "1  2008-02-13       1  \n",
       "2  2007-08-17       1  \n",
       "3  2008-04-17       1  \n",
       "4  2008-10-08       1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing one dataset\n",
    "spark.read.parquet('../0_global_data/fd-cidacs-rl/sinthetic-datasets-b/sinthetic-datasets-b-10000.parquet/').limit(5).toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
